<!DOCTYPE html>
<html lang="fr-FR" xml:lang="fr-FR">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Entropie | Mesures de la Biodiversité</title>
  <meta name="description" content="Définitions, estimation et décompositon de la biodiversité" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Entropie | Mesures de la Biodiversité" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/EricMarcon/MesuresBioDiv2/" />
  <meta property="og:image" content="https://github.com/EricMarcon/MesuresBioDiv2/images/logo.png" />
  <meta property="og:description" content="Définitions, estimation et décompositon de la biodiversité" />
  <meta name="github-repo" content="EricMarcon/MesuresBioDiv2" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Entropie | Mesures de la Biodiversité" />
  
  <meta name="twitter:description" content="Définitions, estimation et décompositon de la biodiversité" />
  <meta name="twitter:image" content="https://github.com/EricMarcon/MesuresBioDiv2/images/logo.png" />

<meta name="author" content="Eric Marcon" />


<meta name="date" content="2021-01-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-MesuresNeutres.html"/>
<link rel="next" href="sec-Equitabilite.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163419632-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163419632-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mesures de la Biodiversité</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Motivation</a></li>
<li class="chapter" data-level="" data-path="calculs-et-données.html"><a href="calculs-et-données.html"><i class="fa fa-check"></i>Calculs et données</a></li>
<li class="chapter" data-level="" data-path="notations.html"><a href="notations.html"><i class="fa fa-check"></i>Notations</a></li>
<li class="part"><span><b>I Notions</b></span></li>
<li class="chapter" data-level="1" data-path="chap-Notions.html"><a href="chap-Notions.html"><i class="fa fa-check"></i><b>1</b> Notions de Diversité</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-Notions.html"><a href="chap-Notions.html#composantes"><i class="fa fa-check"></i><b>1.1</b> Composantes</a><ul>
<li class="chapter" data-level="1.1.1" data-path="chap-Notions.html"><a href="chap-Notions.html#richesse"><i class="fa fa-check"></i><b>1.1.1</b> Richesse</a></li>
<li class="chapter" data-level="1.1.2" data-path="chap-Notions.html"><a href="chap-Notions.html#équitabilité"><i class="fa fa-check"></i><b>1.1.2</b> Équitabilité</a></li>
<li class="chapter" data-level="1.1.3" data-path="chap-Notions.html"><a href="chap-Notions.html#disparité"><i class="fa fa-check"></i><b>1.1.3</b> Disparité</a></li>
<li class="chapter" data-level="1.1.4" data-path="chap-Notions.html"><a href="chap-Notions.html#agrégation"><i class="fa fa-check"></i><b>1.1.4</b> Agrégation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chap-Notions.html"><a href="chap-Notions.html#niveaux-de-létude"><i class="fa fa-check"></i><b>1.2</b> Niveaux de l’étude</a><ul>
<li class="chapter" data-level="1.2.1" data-path="chap-Notions.html"><a href="chap-Notions.html#diversité-alpha-beta-et-gamma"><i class="fa fa-check"></i><b>1.2.1</b> Diversité <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> et <span class="math inline">\(\gamma\)</span></a></li>
<li class="chapter" data-level="1.2.2" data-path="chap-Notions.html"><a href="chap-Notions.html#décomposition"><i class="fa fa-check"></i><b>1.2.2</b> Décomposition</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chap-Notions.html"><a href="chap-Notions.html#le-problème-de-lespèce"><i class="fa fa-check"></i><b>1.3</b> Le problème de l’espèce</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-Outils.html"><a href="chap-Outils.html"><i class="fa fa-check"></i><b>2</b> Outils</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-Outils.html"><a href="chap-Outils.html#courbes-daccumulation"><i class="fa fa-check"></i><b>2.1</b> Courbes d’accumulation</a></li>
<li class="chapter" data-level="2.2" data-path="chap-Outils.html"><a href="chap-Outils.html#diversité-asymptotique"><i class="fa fa-check"></i><b>2.2</b> Diversité asymptotique</a></li>
<li class="chapter" data-level="2.3" data-path="chap-Outils.html"><a href="chap-Outils.html#sec-Couverture"><i class="fa fa-check"></i><b>2.3</b> Taux de couverture</a><ul>
<li class="chapter" data-level="2.3.1" data-path="chap-Outils.html"><a href="chap-Outils.html#formule-des-fréquences-de-good-turing"><i class="fa fa-check"></i><b>2.3.1</b> Formule des fréquences de Good-Turing</a></li>
<li class="chapter" data-level="2.3.2" data-path="chap-Outils.html"><a href="chap-Outils.html#taux-de-couverture-et-déficit-de-couverture"><i class="fa fa-check"></i><b>2.3.2</b> Taux de couverture et déficit de couverture</a></li>
<li class="chapter" data-level="2.3.3" data-path="chap-Outils.html"><a href="chap-Outils.html#complétude"><i class="fa fa-check"></i><b>2.3.3</b> Complétude</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="chap-Outils.html"><a href="chap-Outils.html#distribution-de-labondance-des-espèces-sad"><i class="fa fa-check"></i><b>2.4</b> Distribution de l’abondance des espèces (SAD)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="chap-Outils.html"><a href="chap-Outils.html#la-distribution-en-log-séries"><i class="fa fa-check"></i><b>2.4.1</b> La distribution en log-séries</a></li>
<li class="chapter" data-level="2.4.2" data-path="chap-Outils.html"><a href="chap-Outils.html#la-distribution-broken-stick"><i class="fa fa-check"></i><b>2.4.2</b> La distribution Broken Stick</a></li>
<li class="chapter" data-level="2.4.3" data-path="chap-Outils.html"><a href="chap-Outils.html#la-distribution-log-normale"><i class="fa fa-check"></i><b>2.4.3</b> La distribution log-normale</a></li>
<li class="chapter" data-level="2.4.4" data-path="chap-Outils.html"><a href="chap-Outils.html#la-distribution-géométrique"><i class="fa fa-check"></i><b>2.4.4</b> La distribution géométrique</a></li>
<li class="chapter" data-level="2.4.5" data-path="chap-Outils.html"><a href="chap-Outils.html#synthèse"><i class="fa fa-check"></i><b>2.4.5</b> Synthèse</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Diversité neutre d’une communauté</b></span></li>
<li class="chapter" data-level="3" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html"><i class="fa fa-check"></i><b>3</b> Mesures neutres de la diversité <span class="math inline">\(\alpha\)</span> ou <span class="math inline">\(\gamma\)</span></a><ul>
<li class="chapter" data-level="3.1" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#sec-Richesse"><i class="fa fa-check"></i><b>3.1</b> Richesse spécifique</a><ul>
<li class="chapter" data-level="3.1.1" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#techniques-destimation-non-paramétrique"><i class="fa fa-check"></i><b>3.1.1</b> Techniques d’estimation non paramétrique</a></li>
<li class="chapter" data-level="3.1.2" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#inférence-du-nombre-despèces-à-partir-de-la-sad"><i class="fa fa-check"></i><b>3.1.2</b> Inférence du nombre d’espèces à partir de la SAD</a></li>
<li class="chapter" data-level="3.1.3" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#sec-RichesseSAC"><i class="fa fa-check"></i><b>3.1.3</b> Inférence du nombre d’espèces à partir de courbes d’accumulation</a></li>
<li class="chapter" data-level="3.1.4" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#diversité-générique"><i class="fa fa-check"></i><b>3.1.4</b> Diversité générique</a></li>
<li class="chapter" data-level="3.1.5" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#combien-y-a-t-il-despèces-différentes-sur-terre"><i class="fa fa-check"></i><b>3.1.5</b> Combien y a-t-il d’espèces différentes sur Terre?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#indice-de-simpson"><i class="fa fa-check"></i><b>3.2</b> Indice de Simpson</a><ul>
<li class="chapter" data-level="3.2.1" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#définition"><i class="fa fa-check"></i><b>3.2.1</b> Définition</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#estimation"><i class="fa fa-check"></i><b>3.2.2</b> Estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#indice-de-shannon"><i class="fa fa-check"></i><b>3.3</b> Indice de Shannon</a><ul>
<li class="chapter" data-level="3.3.1" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#définition-1"><i class="fa fa-check"></i><b>3.3.1</b> Définition</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#sec-BiaisShannon"><i class="fa fa-check"></i><b>3.3.2</b> Estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#sec-Hurlbert"><i class="fa fa-check"></i><b>3.4</b> Indice de Hurlbert</a><ul>
<li class="chapter" data-level="3.4.1" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#définition-2"><i class="fa fa-check"></i><b>3.4.1</b> Définition</a></li>
<li class="chapter" data-level="3.4.2" data-path="chap-MesuresNeutres.html"><a href="chap-MesuresNeutres.html#estimation-1"><i class="fa fa-check"></i><b>3.4.2</b> Estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="entropie.html"><a href="entropie.html"><i class="fa fa-check"></i><b>4</b> Entropie</a><ul>
<li class="chapter" data-level="4.1" data-path="entropie.html"><a href="entropie.html#définition-de-lentropie"><i class="fa fa-check"></i><b>4.1</b> Définition de l’entropie</a></li>
<li class="chapter" data-level="4.2" data-path="entropie.html"><a href="entropie.html#entropie-relative"><i class="fa fa-check"></i><b>4.2</b> Entropie relative</a></li>
<li class="chapter" data-level="4.3" data-path="entropie.html"><a href="entropie.html#lappropriation-de-lentropie-par-la-biodiversité"><i class="fa fa-check"></i><b>4.3</b> L’appropriation de l’entropie par la biodiversité</a></li>
<li class="chapter" data-level="4.4" data-path="entropie.html"><a href="entropie.html#entropie-hcdt"><i class="fa fa-check"></i><b>4.4</b> Entropie HCDT</a><ul>
<li class="chapter" data-level="4.4.1" data-path="entropie.html"><a href="entropie.html#logarithmes-déformés"><i class="fa fa-check"></i><b>4.4.1</b> Logarithmes déformés</a></li>
<li class="chapter" data-level="4.4.2" data-path="entropie.html"><a href="entropie.html#entropie-et-diversité"><i class="fa fa-check"></i><b>4.4.2</b> Entropie et diversité</a></li>
<li class="chapter" data-level="4.4.3" data-path="entropie.html"><a href="entropie.html#synthèse-1"><i class="fa fa-check"></i><b>4.4.3</b> Synthèse</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="entropie.html"><a href="entropie.html#profils-de-diversité"><i class="fa fa-check"></i><b>4.5</b> Profils de diversité</a></li>
<li class="chapter" data-level="4.6" data-path="entropie.html"><a href="entropie.html#estimation-de-lentropie"><i class="fa fa-check"></i><b>4.6</b> Estimation de l’entropie</a><ul>
<li class="chapter" data-level="4.6.1" data-path="entropie.html"><a href="entropie.html#zhang-et-grabchak"><i class="fa fa-check"></i><b>4.6.1</b> Zhang et Grabchak</a></li>
<li class="chapter" data-level="4.6.2" data-path="entropie.html"><a href="entropie.html#sec-BiaisHCDT"><i class="fa fa-check"></i><b>4.6.2</b> Chao et Jost</a></li>
<li class="chapter" data-level="4.6.3" data-path="entropie.html"><a href="entropie.html#autres-méthodes"><i class="fa fa-check"></i><b>4.6.3</b> Autres méthodes</a></li>
<li class="chapter" data-level="4.6.4" data-path="entropie.html"><a href="entropie.html#biais-des-estimateurs"><i class="fa fa-check"></i><b>4.6.4</b> Biais des estimateurs</a></li>
<li class="chapter" data-level="4.6.5" data-path="entropie.html"><a href="entropie.html#intervalle-de-confiance-des-estimateurs"><i class="fa fa-check"></i><b>4.6.5</b> Intervalle de confiance des estimateurs</a></li>
<li class="chapter" data-level="4.6.6" data-path="entropie.html"><a href="entropie.html#pratique-de-lestimation"><i class="fa fa-check"></i><b>4.6.6</b> Pratique de l’estimation</a></li>
<li class="chapter" data-level="4.6.7" data-path="entropie.html"><a href="entropie.html#msom-et-msam"><i class="fa fa-check"></i><b>4.6.7</b> MSOM et MSAM</a></li>
<li class="chapter" data-level="4.6.8" data-path="entropie.html"><a href="entropie.html#sec-RarExtrapol"><i class="fa fa-check"></i><b>4.6.8</b> Raréfaction et extrapolation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="entropie.html"><a href="entropie.html#autres-approches"><i class="fa fa-check"></i><b>4.7</b> Autres approches</a><ul>
<li class="chapter" data-level="4.7.1" data-path="entropie.html"><a href="entropie.html#lentropie-de-rényi"><i class="fa fa-check"></i><b>4.7.1</b> L’entropie de Rényi</a></li>
<li class="chapter" data-level="4.7.2" data-path="entropie.html"><a href="entropie.html#lentropie-généralisée-des-mesures-dinégalité"><i class="fa fa-check"></i><b>4.7.2</b> L’entropie généralisée des mesures d’inégalité</a></li>
<li class="chapter" data-level="4.7.3" data-path="entropie.html"><a href="entropie.html#sec-SimpsonG"><i class="fa fa-check"></i><b>4.7.3</b> L’entropie de Simpson généralisée</a></li>
<li class="chapter" data-level="4.7.4" data-path="entropie.html"><a href="entropie.html#lentropie-par-cas"><i class="fa fa-check"></i><b>4.7.4</b> L’entropie par cas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html"><i class="fa fa-check"></i><b>5</b> Équitabilité</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#approche-axiomatique"><i class="fa fa-check"></i><b>5.1</b> Approche axiomatique</a></li>
<li class="chapter" data-level="5.2" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#indice-classiques"><i class="fa fa-check"></i><b>5.2</b> Indice classiques</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#indice-de-bulla"><i class="fa fa-check"></i><b>5.2.1</b> Indice de Bulla</a></li>
<li class="chapter" data-level="5.2.2" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#indice-de-gini"><i class="fa fa-check"></i><b>5.2.2</b> Indice de Gini</a></li>
<li class="chapter" data-level="5.2.3" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#rapports-de-moyennes"><i class="fa fa-check"></i><b>5.2.3</b> Rapports de moyennes</a></li>
<li class="chapter" data-level="5.2.4" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#ratios-de-hill"><i class="fa fa-check"></i><b>5.2.4</b> Ratios de Hill</a></li>
<li class="chapter" data-level="5.2.5" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#synthèse-2"><i class="fa fa-check"></i><b>5.2.5</b> Synthèse</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#indice-alternatifs"><i class="fa fa-check"></i><b>5.3</b> Indice alternatifs</a><ul>
<li class="chapter" data-level="5.3.1" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#variance-de-la-rareté"><i class="fa fa-check"></i><b>5.3.1</b> Variance de la rareté</a></li>
<li class="chapter" data-level="5.3.2" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#non-spécificité"><i class="fa fa-check"></i><b>5.3.2</b> Non-spécificité</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#indice-de-pielou"><i class="fa fa-check"></i><b>5.4</b> Indice de Pielou</a></li>
<li class="chapter" data-level="5.5" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#généralisation-de-lindice-de-pielou"><i class="fa fa-check"></i><b>5.5</b> Généralisation de l’indice de Pielou</a><ul>
<li class="chapter" data-level="5.5.1" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#sec-Mendes"><i class="fa fa-check"></i><b>5.5.1</b> Mendes et al. </a></li>
<li class="chapter" data-level="5.5.2" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#tuomisto"><i class="fa fa-check"></i><b>5.5.2</b> Tuomisto</a></li>
<li class="chapter" data-level="5.5.3" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#jost"><i class="fa fa-check"></i><b>5.5.3</b> Jost</a></li>
<li class="chapter" data-level="5.5.4" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#synthèse-3"><i class="fa fa-check"></i><b>5.5.4</b> Synthèse</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="sec-Equitabilite.html"><a href="sec-Equitabilite.html#relations-empiriques-entre-richesse-et-équitabilité"><i class="fa fa-check"></i><b>5.6</b> Relations empiriques entre richesse et équitabilité</a></li>
</ul></li>
<li class="part"><span><b>III Diversité fonctionnelle et phylogénétique</b></span></li>
<li class="chapter" data-level="6" data-path="chap-cadrephyfonc.html"><a href="chap-cadrephyfonc.html"><i class="fa fa-check"></i><b>6</b> Cadre</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-cadrephyfonc.html"><a href="chap-cadrephyfonc.html#dissimilarité-et-distance"><i class="fa fa-check"></i><b>6.1</b> Dissimilarité et distance</a></li>
<li class="chapter" data-level="6.2" data-path="chap-cadrephyfonc.html"><a href="chap-cadrephyfonc.html#sec-Dphylo"><i class="fa fa-check"></i><b>6.2</b> Distance phylogénétique</a></li>
<li class="chapter" data-level="6.3" data-path="chap-cadrephyfonc.html"><a href="chap-cadrephyfonc.html#sec-DFonctionnelle"><i class="fa fa-check"></i><b>6.3</b> Distance fonctionnelle</a></li>
<li class="chapter" data-level="6.4" data-path="chap-cadrephyfonc.html"><a href="chap-cadrephyfonc.html#équivalence-des-deux-diversités"><i class="fa fa-check"></i><b>6.4</b> Équivalence des deux diversités</a></li>
<li class="chapter" data-level="6.5" data-path="chap-cadrephyfonc.html"><a href="chap-cadrephyfonc.html#typologie-des-mesures"><i class="fa fa-check"></i><b>6.5</b> Typologie des mesures</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mesures-particulières.html"><a href="mesures-particulières.html"><i class="fa fa-check"></i><b>7</b> Mesures particulières</a><ul>
<li class="chapter" data-level="7.1" data-path="mesures-particulières.html"><a href="mesures-particulières.html#richesse-équitabilité-et-divergence-fonctionnelle"><i class="fa fa-check"></i><b>7.1</b> Richesse, équitabilité et divergence fonctionnelle</a><ul>
<li class="chapter" data-level="7.1.1" data-path="mesures-particulières.html"><a href="mesures-particulières.html#richesse-fonctionnelle"><i class="fa fa-check"></i><b>7.1.1</b> Richesse fonctionnelle</a></li>
<li class="chapter" data-level="7.1.2" data-path="mesures-particulières.html"><a href="mesures-particulières.html#equitabilité-fonctionnelle"><i class="fa fa-check"></i><b>7.1.2</b> Equitabilité fonctionnelle</a></li>
<li class="chapter" data-level="7.1.3" data-path="mesures-particulières.html"><a href="mesures-particulières.html#divergence-fonctionnelle"><i class="fa fa-check"></i><b>7.1.3</b> Divergence fonctionnelle</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mesures-particulières.html"><a href="mesures-particulières.html#originalité-richesse-et-équitabilité-phylogénétique"><i class="fa fa-check"></i><b>7.2</b> Originalité, richesse et équitabilité phylogénétique</a><ul>
<li class="chapter" data-level="7.2.1" data-path="mesures-particulières.html"><a href="mesures-particulières.html#mesures-spécifiques-doriginalité"><i class="fa fa-check"></i><b>7.2.1</b> Mesures spécifiques d’originalité</a></li>
<li class="chapter" data-level="7.2.2" data-path="mesures-particulières.html"><a href="mesures-particulières.html#sec-OrigTax"><i class="fa fa-check"></i><b>7.2.2</b> Originalité taxonomique de Ricotta</a></li>
<li class="chapter" data-level="7.2.3" data-path="mesures-particulières.html"><a href="mesures-particulières.html#richesse-phylogénétique"><i class="fa fa-check"></i><b>7.2.3</b> Richesse phylogénétique</a></li>
<li class="chapter" data-level="7.2.4" data-path="mesures-particulières.html"><a href="mesures-particulières.html#indices-de-cadotte"><i class="fa fa-check"></i><b>7.2.4</b> Indices de Cadotte</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="mesures-particulières.html"><a href="mesures-particulières.html#diversité-de-scheiner"><i class="fa fa-check"></i><b>7.3</b> Diversité de Scheiner</a></li>
<li class="chapter" data-level="7.4" data-path="mesures-particulières.html"><a href="mesures-particulières.html#diversité-de-solow-et-polasky"><i class="fa fa-check"></i><b>7.4</b> Diversité de Solow et Polasky</a></li>
<li class="chapter" data-level="7.5" data-path="mesures-particulières.html"><a href="mesures-particulières.html#sec-PDFD"><i class="fa fa-check"></i><b>7.5</b> FD et PD</a></li>
<li class="chapter" data-level="7.6" data-path="mesures-particulières.html"><a href="mesures-particulières.html#sec-Rao"><i class="fa fa-check"></i><b>7.6</b> Indice de Rao</a><ul>
<li class="chapter" data-level="7.6.1" data-path="mesures-particulières.html"><a href="mesures-particulières.html#principe"><i class="fa fa-check"></i><b>7.6.1</b> Principe</a></li>
<li class="chapter" data-level="7.6.2" data-path="mesures-particulières.html"><a href="mesures-particulières.html#formalisation"><i class="fa fa-check"></i><b>7.6.2</b> Formalisation</a></li>
<li class="chapter" data-level="7.6.3" data-path="mesures-particulières.html"><a href="mesures-particulières.html#propriétés"><i class="fa fa-check"></i><b>7.6.3</b> Propriétés</a></li>
<li class="chapter" data-level="7.6.4" data-path="mesures-particulières.html"><a href="mesures-particulières.html#calcul-sous-r"><i class="fa fa-check"></i><b>7.6.4</b> Calcul sous R</a></li>
<li class="chapter" data-level="7.6.5" data-path="mesures-particulières.html"><a href="mesures-particulières.html#sec-MaxTheorique"><i class="fa fa-check"></i><b>7.6.5</b> Maximum théorique</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="mesures-particulières.html"><a href="mesures-particulières.html#diversité-et-moyenne"><i class="fa fa-check"></i><b>7.7</b> Diversité et moyenne</a></li>
<li class="chapter" data-level="7.8" data-path="mesures-particulières.html"><a href="mesures-particulières.html#variations-sur-lentropie-quadratique"><i class="fa fa-check"></i><b>7.8</b> Variations sur l’entropie quadratique</a></li>
<li class="chapter" data-level="7.9" data-path="mesures-particulières.html"><a href="mesures-particulières.html#diversité-fonctionnelle-de-chiu-et-chao"><i class="fa fa-check"></i><b>7.9</b> Diversité fonctionnelle de Chiu et Chao</a></li>
<li class="chapter" data-level="7.10" data-path="mesures-particulières.html"><a href="mesures-particulières.html#sec-HpI1"><i class="fa fa-check"></i><b>7.10</b> <span class="math inline">\(H_p\)</span> et <span class="math inline">\(I_1\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-Phyloentropie.html"><a href="chap-Phyloentropie.html"><i class="fa fa-check"></i><b>8</b> Entropie phylogénétique</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-Phyloentropie.html"><a href="chap-Phyloentropie.html#généralisation-de-lentropie-hcdt"><i class="fa fa-check"></i><b>8.1</b> Généralisation de l’entropie HCDT</a></li>
<li class="chapter" data-level="8.2" data-path="chap-Phyloentropie.html"><a href="chap-Phyloentropie.html#sec-phyloEstimation"><i class="fa fa-check"></i><b>8.2</b> Estimation</a></li>
<li class="chapter" data-level="8.3" data-path="chap-Phyloentropie.html"><a href="chap-Phyloentropie.html#sec-phyloEntopieDiv"><i class="fa fa-check"></i><b>8.3</b> Entropie et diversité</a></li>
<li class="chapter" data-level="8.4" data-path="chap-Phyloentropie.html"><a href="chap-Phyloentropie.html#diversité-individuelle"><i class="fa fa-check"></i><b>8.4</b> Diversité individuelle</a></li>
<li class="chapter" data-level="8.5" data-path="chap-Phyloentropie.html"><a href="chap-Phyloentropie.html#arbres-non-ultramétriques"><i class="fa fa-check"></i><b>8.5</b> Arbres non ultramétriques</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html"><i class="fa fa-check"></i><b>9</b> Diversité de Leinster et Cobbold</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#définitions"><i class="fa fa-check"></i><b>9.1</b> Définitions</a></li>
<li class="chapter" data-level="9.2" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#diversité-neutre"><i class="fa fa-check"></i><b>9.2</b> Diversité neutre</a></li>
<li class="chapter" data-level="9.3" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#diversité-non-neutre"><i class="fa fa-check"></i><b>9.3</b> Diversité non neutre</a></li>
<li class="chapter" data-level="9.4" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#entropie-de-ricotta-et-szeidl"><i class="fa fa-check"></i><b>9.4</b> Entropie de Ricotta et Szeidl</a></li>
<li class="chapter" data-level="9.5" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#définition-de-la-similarité"><i class="fa fa-check"></i><b>9.5</b> Définition de la similarité</a></li>
<li class="chapter" data-level="9.6" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#sec-dqzEstimation"><i class="fa fa-check"></i><b>9.6</b> Estimation</a></li>
<li class="chapter" data-level="9.7" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#diversité-phylogénétique"><i class="fa fa-check"></i><b>9.7</b> Diversité phylogénétique</a></li>
<li class="chapter" data-level="9.8" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#diversité-individuelle-1"><i class="fa fa-check"></i><b>9.8</b> Diversité individuelle</a></li>
<li class="chapter" data-level="9.9" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#diversité-des-valeurs-propres"><i class="fa fa-check"></i><b>9.9</b> Diversité des valeurs propres</a></li>
<li class="chapter" data-level="9.10" data-path="chap-LeinsterCobbold.html"><a href="chap-LeinsterCobbold.html#sec-dqzSynthese"><i class="fa fa-check"></i><b>9.10</b> Synthèse</a></li>
</ul></li>
<li class="part"><span><b>IV Diversité beta et décomposition</b></span></li>
<li class="chapter" data-level="10" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html"><i class="fa fa-check"></i><b>10</b> Cadre</a><ul>
<li class="chapter" data-level="10.1" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html#définition-de-la-diversité-beta-par-objectifs"><i class="fa fa-check"></i><b>10.1</b> Définition de la diversité <span class="math inline">\(\beta\)</span> par objectifs</a></li>
<li class="chapter" data-level="10.2" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html#sec-betaTypo"><i class="fa fa-check"></i><b>10.2</b> Typologie des mesures</a></li>
<li class="chapter" data-level="10.3" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html#sec-betaDecomposition"><i class="fa fa-check"></i><b>10.3</b> Décomposition de la diversité</a><ul>
<li class="chapter" data-level="10.3.1" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html#définitions-de-la-diversité-beta-mesure-dérivée"><i class="fa fa-check"></i><b>10.3.1</b> Définitions de la diversité <span class="math inline">\(\beta\)</span>, mesure dérivée</a></li>
<li class="chapter" data-level="10.3.2" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html#sec-DebatDecomposition"><i class="fa fa-check"></i><b>10.3.2</b> Le débat sur la décomposition</a></li>
<li class="chapter" data-level="10.3.3" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html#décomposition-multiplicative-de-la-diversité"><i class="fa fa-check"></i><b>10.3.3</b> Décomposition multiplicative de la diversité</a></li>
<li class="chapter" data-level="10.3.4" data-path="sec-betaCadre.html"><a href="sec-betaCadre.html#sec-defalpha"><i class="fa fa-check"></i><b>10.3.4</b> Définitions de la diversité <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html"><i class="fa fa-check"></i><b>11</b> Diversité de différentiation</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#données-de-présence-absence"><i class="fa fa-check"></i><b>11.1</b> Données de présence-absence</a><ul>
<li class="chapter" data-level="11.1.1" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#indices-de-similarité"><i class="fa fa-check"></i><b>11.1.1</b> Indices de similarité</a></li>
<li class="chapter" data-level="11.1.2" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#biais-destimation"><i class="fa fa-check"></i><b>11.1.2</b> Biais d’estimation</a></li>
<li class="chapter" data-level="11.1.3" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#représentation-graphique"><i class="fa fa-check"></i><b>11.1.3</b> Représentation graphique</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#données-dabondance"><i class="fa fa-check"></i><b>11.2</b> Données d’abondance</a></li>
<li class="chapter" data-level="11.3" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#emboîtement-et-substitution"><i class="fa fa-check"></i><b>11.3</b> Emboîtement et substitution</a></li>
<li class="chapter" data-level="11.4" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#données-dabondance-et-similarité-des-espèces"><i class="fa fa-check"></i><b>11.4</b> Données d’abondance et similarité des espèces</a><ul>
<li class="chapter" data-level="11.4.1" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#dissimilarité-minimale-moyenne"><i class="fa fa-check"></i><b>11.4.1</b> Dissimilarité minimale moyenne</a></li>
<li class="chapter" data-level="11.4.2" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#similarité-moyenne-entre-individus"><i class="fa fa-check"></i><b>11.4.2</b> Similarité moyenne entre individus</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#distance-entre-paires-de-communautés-et-diversité-beta-proportionnelle"><i class="fa fa-check"></i><b>11.5</b> Distance entre paires de communautés et diversité <span class="math inline">\(\beta\)</span> proportionnelle</a><ul>
<li class="chapter" data-level="11.5.1" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#diversité-de-simpson"><i class="fa fa-check"></i><b>11.5.1</b> Diversité de Simpson</a></li>
<li class="chapter" data-level="11.5.2" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#généralisation"><i class="fa fa-check"></i><b>11.5.2</b> Généralisation</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#sec-TestBetaDiff"><i class="fa fa-check"></i><b>11.6</b> Tests de significativité</a><ul>
<li class="chapter" data-level="11.6.1" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#test-de-non-nullité"><i class="fa fa-check"></i><b>11.6.1</b> Test de non-nullité</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap-BetaPaires.html"><a href="chap-BetaPaires.html#analyse-de-la-variabilité"><i class="fa fa-check"></i><b>11.6.2</b> Analyse de la variabilité</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html"><i class="fa fa-check"></i><b>12</b> Décomposition de l’entropie HCDT</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#décomposition-de-lindice-de-shannon"><i class="fa fa-check"></i><b>12.1</b> Décomposition de l’indice de Shannon</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#définition-3"><i class="fa fa-check"></i><b>12.1.1</b> Définition</a></li>
<li class="chapter" data-level="12.1.2" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#sec-BiaisShannonBeta"><i class="fa fa-check"></i><b>12.1.2</b> Biais d’estimation</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#décomposition-de-lentropie-généralisée"><i class="fa fa-check"></i><b>12.2</b> Décomposition de l’entropie généralisée</a></li>
<li class="chapter" data-level="12.3" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#décomposition-du-nombre-despèces"><i class="fa fa-check"></i><b>12.3</b> Décomposition du nombre d’espèces</a></li>
<li class="chapter" data-level="12.4" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#décomposition-de-lindice-de-gini-simpson"><i class="fa fa-check"></i><b>12.4</b> Décomposition de l’indice de Gini-Simpson</a></li>
<li class="chapter" data-level="12.5" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#sec-RaoDisc"><i class="fa fa-check"></i><b>12.5</b> Décomposition de l’indice de Rao</a></li>
<li class="chapter" data-level="12.6" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#synthèse-4"><i class="fa fa-check"></i><b>12.6</b> Synthèse</a></li>
<li class="chapter" data-level="12.7" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#normalisation"><i class="fa fa-check"></i><b>12.7</b> Normalisation</a><ul>
<li class="chapter" data-level="12.7.1" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#nécessité-de-normaliser-la-diversité-beta"><i class="fa fa-check"></i><b>12.7.1</b> Nécessité de normaliser la diversité <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="12.7.2" data-path="chap-DedompHCDT.html"><a href="chap-DedompHCDT.html#sec-Chevauchement"><i class="fa fa-check"></i><b>12.7.2</b> L’indice de chevauchement <span class="math inline">\(C_{qN}\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="décomposition-de-la-diversité-phylogénétique-et-fonctionnelle.html"><a href="décomposition-de-la-diversité-phylogénétique-et-fonctionnelle.html"><i class="fa fa-check"></i><b>13</b> Décomposition de la diversité phylogénétique et fonctionnelle</a><ul>
<li class="chapter" data-level="13.1" data-path="décomposition-de-la-diversité-phylogénétique-et-fonctionnelle.html"><a href="décomposition-de-la-diversité-phylogénétique-et-fonctionnelle.html#décomposition-de-la-diversité-phylogénétique"><i class="fa fa-check"></i><b>13.1</b> Décomposition de la diversité phylogénétique</a></li>
<li class="chapter" data-level="13.2" data-path="décomposition-de-la-diversité-phylogénétique-et-fonctionnelle.html"><a href="décomposition-de-la-diversité-phylogénétique-et-fonctionnelle.html#partitionnement-de-la-diversité-de-leinster-et-cobbold"><i class="fa fa-check"></i><b>13.2</b> Partitionnement de la diversité de Leinster et Cobbold</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="tests-de-significativité.html"><a href="tests-de-significativité.html"><i class="fa fa-check"></i><b>14</b> Tests de significativité</a><ul>
<li class="chapter" data-level="14.1" data-path="tests-de-significativité.html"><a href="tests-de-significativité.html#test-de-non-nullité-1"><i class="fa fa-check"></i><b>14.1</b> Test de non-nullité</a><ul>
<li class="chapter" data-level="14.1.1" data-path="tests-de-significativité.html"><a href="tests-de-significativité.html#intervalle-de-confiance"><i class="fa fa-check"></i><b>14.1.1</b> Intervalle de confiance</a></li>
<li class="chapter" data-level="14.1.2" data-path="tests-de-significativité.html"><a href="tests-de-significativité.html#correction-des-biais"><i class="fa fa-check"></i><b>14.1.2</b> Correction des biais</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="tests-de-significativité.html"><a href="tests-de-significativité.html#analyse-de-la-variabilité-1"><i class="fa fa-check"></i><b>14.2</b> Analyse de la variabilité</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="sec-betaAutres.html"><a href="sec-betaAutres.html"><i class="fa fa-check"></i><b>15</b> Autres approches</a><ul>
<li class="chapter" data-level="15.1" data-path="sec-betaAutres.html"><a href="sec-betaAutres.html#le-partitionnement-de-la-diversité-selon-pélissier-et-couteron"><i class="fa fa-check"></i><b>15.1</b> Le partitionnement de la diversité selon Pélissier et Couteron</a><ul>
<li class="chapter" data-level="15.1.1" data-path="sec-betaAutres.html"><a href="sec-betaAutres.html#sec-PCcadre"><i class="fa fa-check"></i><b>15.1.1</b> Cadre</a></li>
<li class="chapter" data-level="15.1.2" data-path="sec-betaAutres.html"><a href="sec-betaAutres.html#sec-PCdecomposition"><i class="fa fa-check"></i><b>15.1.2</b> Décomposition</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="sec-betaAutres.html"><a href="sec-betaAutres.html#diversité-de-kosman"><i class="fa fa-check"></i><b>15.2</b> Diversité de Kosman</a></li>
</ul></li>
<li class="part"><span><b>V Diversité jointe, diversité structurelle</b></span></li>
<li class="chapter" data-level="16" data-path="sec-jointeCadre.html"><a href="sec-jointeCadre.html"><i class="fa fa-check"></i><b>16</b> Cadre</a></li>
<li class="chapter" data-level="17" data-path="information-mutuelle.html"><a href="information-mutuelle.html"><i class="fa fa-check"></i><b>17</b> Information mutuelle</a><ul>
<li class="chapter" data-level="17.1" data-path="information-mutuelle.html"><a href="information-mutuelle.html#récursivité-de-lentropie"><i class="fa fa-check"></i><b>17.1</b> Récursivité de l’entropie</a><ul>
<li class="chapter" data-level="17.1.1" data-path="information-mutuelle.html"><a href="information-mutuelle.html#sec-jointeShannon"><i class="fa fa-check"></i><b>17.1.1</b> Entropie de Shannon</a></li>
<li class="chapter" data-level="17.1.2" data-path="information-mutuelle.html"><a href="information-mutuelle.html#sec-jointeHCDT"><i class="fa fa-check"></i><b>17.1.2</b> Entropie HCDT</a></li>
<li class="chapter" data-level="17.1.3" data-path="information-mutuelle.html"><a href="information-mutuelle.html#intérêt-de-linformation-mutuelle"><i class="fa fa-check"></i><b>17.1.3</b> Intérêt de l’information mutuelle</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="information-mutuelle.html"><a href="information-mutuelle.html#taille-de-niche"><i class="fa fa-check"></i><b>17.2</b> Taille de niche</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="chap-DiversiteJointe.html"><a href="chap-DiversiteJointe.html"><i class="fa fa-check"></i><b>18</b> Décomposition de la diversité jointe</a><ul>
<li class="chapter" data-level="18.1" data-path="chap-DiversiteJointe.html"><a href="chap-DiversiteJointe.html#sec-Definitions"><i class="fa fa-check"></i><b>18.1</b> Définitions</a></li>
<li class="chapter" data-level="18.2" data-path="chap-DiversiteJointe.html"><a href="chap-DiversiteJointe.html#relations-avec-dautres-mesures-de-diversité"><i class="fa fa-check"></i><b>18.2</b> Relations avec d’autres mesures de diversité</a></li>
</ul></li>
<li class="part"><span><b>VI Diversité spatialement explicite</b></span></li>
<li class="chapter" data-level="19" data-path="sec-spatialCadre.html"><a href="sec-spatialCadre.html"><i class="fa fa-check"></i><b>19</b> Cadre</a></li>
<li class="chapter" data-level="20" data-path="chap-Fisher.html"><a href="chap-Fisher.html"><i class="fa fa-check"></i><b>20</b> Indice <span class="math inline">\(\alpha\)</span> de Fisher</a><ul>
<li class="chapter" data-level="20.1" data-path="chap-Fisher.html"><a href="chap-Fisher.html#construction"><i class="fa fa-check"></i><b>20.1</b> Construction</a></li>
<li class="chapter" data-level="20.2" data-path="chap-Fisher.html"><a href="chap-Fisher.html#variantes"><i class="fa fa-check"></i><b>20.2</b> Variantes</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="chap-Accumulation.html"><a href="chap-Accumulation.html"><i class="fa fa-check"></i><b>21</b> Accumulation de la diversité locale</a><ul>
<li class="chapter" data-level="21.1" data-path="chap-Accumulation.html"><a href="chap-Accumulation.html#sec-SAC"><i class="fa fa-check"></i><b>21.1</b> Courbes d’accumulation</a></li>
<li class="chapter" data-level="21.2" data-path="chap-Accumulation.html"><a href="chap-Accumulation.html#sec-Extrapolation"><i class="fa fa-check"></i><b>21.2</b> Extrapolation</a></li>
<li class="chapter" data-level="21.3" data-path="chap-Accumulation.html"><a href="chap-Accumulation.html#influence-de-la-structure-spatiale"><i class="fa fa-check"></i><b>21.3</b> Influence de la structure spatiale</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="diversité-régionale.html"><a href="diversité-régionale.html"><i class="fa fa-check"></i><b>22</b> Diversité régionale</a><ul>
<li class="chapter" data-level="22.1" data-path="diversité-régionale.html"><a href="diversité-régionale.html#sec-Arrhenius"><i class="fa fa-check"></i><b>22.1</b> La relation d’Arrhenius</a><ul>
<li class="chapter" data-level="22.1.1" data-path="diversité-régionale.html"><a href="diversité-régionale.html#loi-de-puissance"><i class="fa fa-check"></i><b>22.1.1</b> Loi de puissance</a></li>
<li class="chapter" data-level="22.1.2" data-path="diversité-régionale.html"><a href="diversité-régionale.html#le-modèle-de-plotkin"><i class="fa fa-check"></i><b>22.1.2</b> Le modèle de Plotkin</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="diversité-régionale.html"><a href="diversité-régionale.html#sec-Gleason"><i class="fa fa-check"></i><b>22.2</b> Le modèle de Gleason</a></li>
<li class="chapter" data-level="22.3" data-path="diversité-régionale.html"><a href="diversité-régionale.html#le-modèle-de-placement-aléatoire"><i class="fa fa-check"></i><b>22.3</b> Le modèle de placement aléatoire</a></li>
<li class="chapter" data-level="22.4" data-path="diversité-régionale.html"><a href="diversité-régionale.html#sec-spatialSynthese"><i class="fa fa-check"></i><b>22.4</b> Synthèse</a></li>
<li class="chapter" data-level="22.5" data-path="diversité-régionale.html"><a href="diversité-régionale.html#estimation-de-la-richesse-à-partir-de-placettes-distantes"><i class="fa fa-check"></i><b>22.5</b> Estimation de la richesse à partir de placettes distantes</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="chap-betasp.html"><a href="chap-betasp.html"><i class="fa fa-check"></i><b>23</b> Diversité <span class="math inline">\(\beta\)</span> spatialement explicite</a><ul>
<li class="chapter" data-level="23.1" data-path="chap-betasp.html"><a href="chap-betasp.html#sec-SimpsonSpatial"><i class="fa fa-check"></i><b>23.1</b> L’indice de Simpson spatialement explicite</a></li>
<li class="chapter" data-level="23.2" data-path="chap-betasp.html"><a href="chap-betasp.html#le-variogramme-de-complémentarité"><i class="fa fa-check"></i><b>23.2</b> Le variogramme de complémentarité</a></li>
</ul></li>
<li class="part"><span><b>VII Conclusion</b></span></li>
<li class="chapter" data-level="24" data-path="sec-conclusionSynthese.html"><a href="sec-conclusionSynthese.html"><i class="fa fa-check"></i><b>24</b> Synthèse</a><ul>
<li class="chapter" data-level="24.1" data-path="sec-conclusionSynthese.html"><a href="sec-conclusionSynthese.html#entropie-et-diversité-1"><i class="fa fa-check"></i><b>24.1</b> Entropie et diversité</a></li>
<li class="chapter" data-level="24.2" data-path="sec-conclusionSynthese.html"><a href="sec-conclusionSynthese.html#ordres-particuliers-de-diversité"><i class="fa fa-check"></i><b>24.2</b> Ordres particuliers de diversité</a><ul>
<li class="chapter" data-level="24.2.1" data-path="sec-conclusionSynthese.html"><a href="sec-conclusionSynthese.html#estimation-2"><i class="fa fa-check"></i><b>24.2.1</b> Estimation</a></li>
<li class="chapter" data-level="24.2.2" data-path="sec-conclusionSynthese.html"><a href="sec-conclusionSynthese.html#partition"><i class="fa fa-check"></i><b>24.2.2</b> Partition</a></li>
<li class="chapter" data-level="24.2.3" data-path="sec-conclusionSynthese.html"><a href="sec-conclusionSynthese.html#égalité-des-diversités"><i class="fa fa-check"></i><b>24.2.3</b> Égalité des diversités</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="sec-conclusionSynthese.html"><a href="sec-conclusionSynthese.html#limites"><i class="fa fa-check"></i><b>24.3</b> Limites</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/EricMarcon/MesuresBioDiv2" target="blank">Publié avec bookdown, hébergé sur GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mesures de la Biodiversité</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="entropie" class="section level1">
<h1><span class="header-section-number">4</span> Entropie</h1>
<p></p>
<div class="Summary">
<p>
L’entropie est la surprise moyenne apportée par l’observation des individus d’une communauté, d’autant plus grande qu’un individu appartient à une espèce plus rare. L’entropie HCDT permet d’unifier les indices classiques de diversité: son paramètre, appelé ordre, fixe l’importance donnée aux espèces rares. L’entropie d’ordre 0 est la richesse; celle d’ordre 1, l’indice de Shannon; celle d’ordre 2, celui de Simpson. L’entropie est la moyenne du logarithme déformé de la rareté des espèces, définie comme l’inverse de leur probabilité.
</p>
<p>
L’entropie va de pair avec la diversité au sens strict (Nombres de Hill): le nombre d’espèces équiprobables dont l’entropie est la même que celle de la communauté réelle. La diversité est l’exponentielle déformée de l’entropie. Les profils de diversité représentent la diversité en fonction de son ordre et permettent la comparaison de communautés.
</p>
<p>
L’estimation de la diversité est difficile pour des ordres inférieurs à <span class="math inline"><span class="math inline">\(0,5\)</span></span> dans des taxocènes très divers comme les arbres des forêts tropicales.
</p>
</div>
<p></p>
<p>L’entropie peut être entendue comme la surprise moyenne fournie par l’observation d’un échantillon.
C’est intuitivement une bonne mesure de diversité <span class="citation">(Pielou <a href="#ref-Pielou1975" role="doc-biblioref">1975</a>)</span>.
Ses propriétés mathématiques permettent d’unifier les mesures de diversité dans un cadre général.</p>
<div id="définition-de-lentropie" class="section level2">
<h2><span class="header-section-number">4.1</span> Définition de l’entropie</h2>
<p>Les textes fondateurs sont <span class="citation">Davis (<a href="#ref-Davis1941" role="doc-biblioref">1941</a>)</span> et surtout <span class="citation">Theil (<a href="#ref-Theil1967" role="doc-biblioref">1967</a>)</span> en économétrie, et Shannon <span class="citation">(<a href="#ref-Shannon1948" role="doc-biblioref">1948</a>; <a href="#ref-Shannon1963" role="doc-biblioref">1963</a>)</span> pour la mesure de la diversité.
Une revue est fournie par <span class="citation">Maasoumi (<a href="#ref-Maasoumi1993" role="doc-biblioref">1993</a>)</span>.</p>
<p>Considérons une expérience dont les résultats possibles sont <span class="math inline">\(\left\{r_1,r_2,\dots ,\ r_S\right\}\)</span>.
La probabilité d’obtenir <span class="math inline">\(r_s\)</span> est <span class="math inline">\(p_s\)</span>, et <span class="math inline">\(\mathbf{p}=(p_1,p_2,\dots,p_S)\)</span> est le vecteur composé des probabilités d’obtenir chaque résultat.
Les probabilités sont connues <em>a priori</em>.
Tout ce qui suit est vrai aussi pour des valeurs de <span class="math inline">\(r\)</span> continues, dont on connaîtrait la densité de probabilité.</p>
<p>On considère maintenant un échantillon de valeurs de <span class="math inline">\(r\)</span>.
La présence de <span class="math inline">\(r_s\)</span> dans l’échantillon est peu étonnante si <span class="math inline">\(p_s\)</span> est grande: elle apporte peu d’information supplémentaire par rapport à la simple connaissance des probabilités.
En revanche, si <span class="math inline">\(p_s\)</span> est petite, la présence de <span class="math inline">\(r_s\)</span> est surprenante.
On définit donc une fonction d’information, <span class="math inline">\(I(p_s)\)</span>, décroissante quand la probabilité augmente, de <span class="math inline">\(I(0)&gt;0\)</span> (éventuellement <span class="math inline">\(+\infty\)</span>) à <span class="math inline">\(I(1)=0\)</span>.
Chaque valeur observée dans l’échantillon apporte une certaine quantité d’information, dont la somme est l’information de l’échantillon.
<span class="citation">Patil et Taillie (<a href="#ref-Patil1982" role="doc-biblioref">1982</a>)</span> appellent l’information “rareté”.</p>
<p>La quantité d’information attendue de l’expérience est <span class="math inline">\(\sum^S_{s=1}{p_s I(p_s) = H(\mathbf{p})}\)</span>.
Si on choisit <span class="math inline">\(I\left(p_s\right)=-\ln\left(p_s\right)\)</span>, <span class="math inline">\(H\left(\mathbf{p}\right)\)</span> est l’indice de Shannon, mais bien d’autres formes de <span class="math inline">\(I\left(p_s\right)\)</span> sont possibles.
<span class="math inline">\(H\left(\mathbf{p}\right)\)</span> est appelée <em>entropie</em>.
C’est une mesure de l’incertitude (de la volatilité) du résultat de l’expérience.
Si le résultat est certain (une seule valeur <span class="math inline">\(p_S\)</span> vaut 1), l’entropie est nulle.
L’entropie est maximale quand les résultats sont équiprobables.</p>
<p>Si <span class="math inline">\(\mathbf{p}\)</span> est la distribution des probabilité des espèces dans une communauté, <span class="citation">Patil et Taillie (<a href="#ref-Patil1982" role="doc-biblioref">1982</a>)</span> montrent que:</p>
<ul>
<li>Si <span class="math inline">\(I\left(p_s\right) = (1-p_s)/{p_s}\)</span>, alors <span class="math inline">\(H\left(\mathbf{p}\right)\)</span> est le nombre d’espèces <span class="math inline">\(S\)</span> moins 1;</li>
<li>Si <span class="math inline">\(I\left(p_s\right)=-\ln\left(p_s\right)\)</span>, alors <span class="math inline">\(H\left(\mathbf{p}\right)\)</span> est l’indice de Shannon;</li>
<li>Si <span class="math inline">\(I\left(p_s\right)=1-p_s\)</span>, alors <span class="math inline">\(H\left(\mathbf{p}\right)\)</span> est l’indice de Simpson.</li>
</ul>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:IFig"></span>
<img src="MesuresBD_files/figure-html/IFig-1.png" alt="Fonctions d’information utilisées dans le nombre d’espèces (trait plein), l’indice de Shannon (pointillés longs) et l’indice de Simpson (pointillés). L’information apportée par l’observation d’espèces rares décroît du nombre d’espèces à l’indice de Simpson." width="80%" />
<p class="caption">
Figure 4.1: Fonctions d’information utilisées dans le nombre d’espèces (trait plein), l’indice de Shannon (pointillés longs) et l’indice de Simpson (pointillés). L’information apportée par l’observation d’espèces rares décroît du nombre d’espèces à l’indice de Simpson.
</p>
</div>
<p></p>
<p>Ces trois fonctions d’information sont représentées en figure <a href="entropie.html#fig:IFig">4.1</a>.</p>
<p>Le code R nécessaire pour réaliser la figure est:</p>
<p></p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="entropie.html#cb147-1"></a>I0 &lt;-<span class="st"> </span><span class="cf">function</span>(p) (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p)<span class="op">/</span>p</span>
<span id="cb147-2"><a href="entropie.html#cb147-2"></a>I1 &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="op">-</span><span class="kw">log</span>(p)</span>
<span id="cb147-3"><a href="entropie.html#cb147-3"></a>I2 &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p</span>
<span id="cb147-4"><a href="entropie.html#cb147-4"></a><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> I0) <span class="op">+</span></span>
<span id="cb147-5"><a href="entropie.html#cb147-5"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> I1, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> I2,</span>
<span id="cb147-6"><a href="entropie.html#cb147-6"></a>    <span class="dt">lty =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>)) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;p&quot;</span>,</span>
<span id="cb147-7"><a href="entropie.html#cb147-7"></a>    <span class="dt">y =</span> <span class="st">&quot;I(p)&quot;</span>)</span></code></pre></div>
<p></p>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:EntropieFig"></span>
<img src="MesuresBD_files/figure-html/EntropieFig-1.png" alt="Valeur de \(p_{s}I(p_s)\) dans le nombre d’espèces (trait plein), l’indice de Shannon (pointillés longs) et l’indice de Simpson (pointillés). Les espèces rares contribuent peu, sauf pour le nombre d’espèces." width="80%" />
<p class="caption">
Figure 4.2: Valeur de <span class="math inline">\(p_{s}I(p_s)\)</span> dans le nombre d’espèces (trait plein), l’indice de Shannon (pointillés longs) et l’indice de Simpson (pointillés). Les espèces rares contribuent peu, sauf pour le nombre d’espèces.
</p>
</div>
<p></p>
<p>La contribution de chaque espèce à la valeur totale de l’entropie est représentée figure <a href="entropie.html#fig:EntropieFig">4.2</a>.</p>
<p>Code R:</p>
<p></p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="entropie.html#cb148-1"></a>H0 &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p</span>
<span id="cb148-2"><a href="entropie.html#cb148-2"></a>H1 &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="op">-</span>p <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(p)</span>
<span id="cb148-3"><a href="entropie.html#cb148-3"></a>H2 &lt;-<span class="st"> </span><span class="cf">function</span>(p) p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p)</span>
<span id="cb148-4"><a href="entropie.html#cb148-4"></a><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">1</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> H0) <span class="op">+</span></span>
<span id="cb148-5"><a href="entropie.html#cb148-5"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> H1, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> H2,</span>
<span id="cb148-6"><a href="entropie.html#cb148-6"></a>    <span class="dt">lty =</span> <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;H(p)&quot;</span>)</span></code></pre></div>
<p></p>
</div>
<div id="entropie-relative" class="section level2">
<h2><span class="header-section-number">4.2</span> Entropie relative</h2>
<p>Considérons maintenant les probabilités <span class="math inline">\(q_s\)</span> formant l’ensemble <span class="math inline">\(\mathbf{q}\)</span> obtenues par la réalisation de l’expérience.
Elles sont différentes des probabilités <span class="math inline">\(p_s\)</span>, par exemple parce que l’expérience ne s’est pas déroulée exactement comme prévu.
On définit le gain d’information <span class="math inline">\(I(q_s,p_s)\)</span> comme la quantité d’information supplémentaire fournie par l’observation d’un résultat de l’expérience, connaissant les probabilités <em>a priori</em>.
La quantité totale d’information fournie par l’expérience, <span class="math inline">\(\sum^S_{s=1}{q_sI(q_s,p_s)}=H(\mathbf{q},\mathbf{p})\)</span>, est souvent appelée entropie relative.
Elle peut être vue comme une distance entre la distribution <em>a priori</em> et la distribution <em>a posteriori</em>.
Il est possible que les distributions <span class="math inline">\(\mathbf{p}\)</span> et <span class="math inline">\(\mathbf{q}\)</span> soit identiques, que le gain d’information soit donc nul, mais les estimateurs empiriques n’étant pas exactement égaux entre eux, des tests de significativité de la valeur de <span class="math inline">\(\hat{H}(\mathbf{q},\mathbf{p})\)</span> seront nécessaires.</p>
<p>Quelques formes possibles de <span class="math inline">\(H(\mathbf{q},\mathbf{p})\)</span> sont:</p>
<ul>
<li>La divergence de Kullback-Leibler <span class="citation">(Kullback et Leibler <a href="#ref-Kullback1951" role="doc-biblioref">1951</a>)</span> connue par les économistes comme l’indice de dissimilarité de <span class="citation">Theil (<a href="#ref-Theil1967" role="doc-biblioref">1967</a>)</span>:
<span class="math display" id="eq:Theil">\[\begin{equation}
\tag{4.1}
T = \sum^S_{s=1}{q_{s}\ln\frac{q_s}{p_s}};
\end{equation}\]</span></li>
<li>Sa proche parente, appelée parfois deuxième mesure de Theil <span class="citation">(Conceição et Ferreira <a href="#ref-Conceicao2000" role="doc-biblioref">2000</a>)</span>, qui inverse simplement les rôles de <span class="math inline">\(p\)</span> et <span class="math inline">\(q\)</span>:
<span class="math display" id="eq:Theil2">\[\begin{equation}
\tag{4.2}
L = \sum^S_{s=1}{p_{s}\ln\frac{p_s}{q_s}}.
\end{equation}\]</span></li>
</ul>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:KullbackLeiblerFig"></span>
<img src="MesuresBD_files/figure-html/KullbackLeiblerFig-1.png" alt="Valeur de \(q\ln(q/p)\) en fonction de \(p\) et \(q\). La divergence de Kullback-Leibler est la somme de cette valeur pour toutes les espèces" width="80%" />
<p class="caption">
Figure 4.3: Valeur de <span class="math inline">\(q\ln(q/p)\)</span> en fonction de <span class="math inline">\(p\)</span> et <span class="math inline">\(q\)</span>. La divergence de Kullback-Leibler est la somme de cette valeur pour toutes les espèces
</p>
</div>
<p></p>
<p>L’entropie relative est essentielle pour la définition de la diversité <span class="math inline">\(\beta\)</span> présentée dans le chapitre <a href="chap-DedompHCDT.html#chap-DedompHCDT">12</a>.
En se limitant à la diversité <span class="math inline">\(\alpha\)</span>, on peut remarquer que l’indice de Shannon est la divergence de Kullback-Leibler entre la distribution observée et l’équiprobabilité des espèces <span class="citation">(Marcon et al. <a href="#ref-Marcon2012a" role="doc-biblioref">2012</a>)</span>.
Les valeurs de chaque terme de la divergence sont représentées en figure <a href="entropie.html#fig:KullbackLeiblerFig">4.3</a>.</p>
<p>Le code R nécessaire pour réaliser la figure est:</p>
<p></p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="entropie.html#cb149-1"></a>p &lt;-<span class="st"> </span>q &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="fl">0.01</span>)</span>
<span id="cb149-2"><a href="entropie.html#cb149-2"></a>KB &lt;-<span class="st"> </span><span class="cf">function</span>(p, q) p <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(p<span class="op">/</span>q)</span>
<span id="cb149-3"><a href="entropie.html#cb149-3"></a>xyz &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">outer</span>(p, q, <span class="dt">FUN =</span> <span class="st">&quot;KB&quot;</span>))</span>
<span id="cb149-4"><a href="entropie.html#cb149-4"></a><span class="kw">library</span>(<span class="st">&quot;sp&quot;</span>)</span>
<span id="cb149-5"><a href="entropie.html#cb149-5"></a><span class="kw">image</span>(xyz, <span class="dt">col =</span> <span class="kw">bpy.colors</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">cutoff.tails =</span> <span class="fl">0.3</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>),</span>
<span id="cb149-6"><a href="entropie.html#cb149-6"></a>    <span class="dt">xlab =</span> <span class="st">&quot;q&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">asp =</span> <span class="dv">1</span>)</span>
<span id="cb149-7"><a href="entropie.html#cb149-7"></a><span class="kw">contour</span>(xyz, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="fl">0.3</span>, <span class="dv">0</span>, <span class="fl">0.1</span>), <span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>), <span class="kw">seq</span>(<span class="dv">1</span>,</span>
<span id="cb149-8"><a href="entropie.html#cb149-8"></a>    <span class="dv">4</span>, <span class="dv">1</span>)), <span class="dt">labcex =</span> <span class="dv">1</span>, <span class="dt">add =</span> T)</span></code></pre></div>
<p></p>
</div>
<div id="lappropriation-de-lentropie-par-la-biodiversité" class="section level2">
<h2><span class="header-section-number">4.3</span> L’appropriation de l’entropie par la biodiversité</h2>
<p><span class="citation">MacArthur (<a href="#ref-MacArthur1955" role="doc-biblioref">1955</a>)</span> est le premier à avoir introduit la théorie de l’information en écologie <span class="citation">(Ulanowicz <a href="#ref-Ulanowicz2001" role="doc-biblioref">2001</a>)</span>.
MacArthur s’intéressait aux réseaux trophiques et cherchait à mesurer leur stabilité: l’indice de Shannon qui comptabilise le nombre de relations possibles lui paraissait une bonne façon de l’évaluer.
Mais l’efficacité implique la spécialisation, ignorée dans <span class="math inline">\(H\)</span> qui est une mesure neutre (toutes les espèces y jouent le même rôle).
MacArthur a abandonné cette voie.</p>
<p>Les premiers travaux consistant à généraliser l’indice de Shannon sont dus à <span class="citation">Rényi (<a href="#ref-Renyi1961" role="doc-biblioref">1961</a>)</span>.
L’entropie d’ordre <span class="math inline">\(q\)</span> de Rényi est</p>
<p><span class="math display">\[\begin{equation}
^{q}\!R =\frac{1}{1-q\ln\sum^S_{q=1}{p^q_s}}.
\end{equation}\]</span></p>
<p>Rényi pose également les axiomes pour une mesure d’entropie <span class="math inline">\(R\left(\mathbf{p}\right)\)</span>, où <span class="math inline">\(\mathbf{p}=(p_1,p_2,\dots,p_S)\)</span>:</p>
<ul>
<li>La symétrie: les espèces doivent être interchangeables, aucune n’a de rôle particulier et leur ordre est indifférent;</li>
<li>La mesure doit être continue par rapport aux probabilités;</li>
<li>La valeur maximale est atteinte si toutes les probabilités sont égales.</li>
</ul>
<p>Il montre que <span class="math inline">\(^{q}\!R\)</span> respecte les 3 axiomes.</p>
<p><span class="citation">Patil et Taillie (<a href="#ref-Patil1982" role="doc-biblioref">1982</a>)</span> ont montré de plus que:</p>
<ul>
<li>L’introduction d’une espèce dans une communauté augmente sa diversité (conséquence de la décroissance de <span class="math inline">\(g(p_s)\)</span>);</li>
<li>Le remplacement d’un individu d’une espèce fréquente par un individu d’une espèce plus rare augmente l’entropie à condition que <span class="math inline">\(R(\mathbf{p})\)</span> soit concave.
Dans la littérature économique sur les inégalités, cette propriété est connue sous le nom de Pigou-Dalton <span class="citation">(Dalton <a href="#ref-Dalton1920" role="doc-biblioref">1920</a>)</span>.</li>
</ul>
<p><span class="citation">Hill (<a href="#ref-Hill1973" role="doc-biblioref">1973</a>)</span> transforme l’entropie de Rényi en <em>nombres de Hill</em>, qui en sont simplement l’exponentielle:</p>
<p><span class="math display" id="eq:Hill1973">\[\begin{equation}
  \tag{4.3}
  ^{q}\!D = {\left(\sum^S_{s=1}{p^q_s}\right)}^{\frac{1}{1-q}}.
\end{equation}\]</span></p>
<p>Le souci de Hill était de rendre les indices de diversité intelligibles après l’article remarqué de <span class="citation">Hurlbert (<a href="#ref-Hurlbert1971" role="doc-biblioref">1971</a>)</span> intitulé “le non-concept de diversité spécifique”.
Hurlbert reprochait à la littérature sur la diversité sa trop grande abstraction et son éloignement des réalités biologiques, notamment en fournissant des exemples dans lesquels l’ordre des communautés n’est pas le même selon l’indice de diversité choisi.
Les nombres de Hill sont le nombre d’espèces équiprobables donnant la même valeur de diversité que la distribution observée.
Ils sont des transformations simples des indices classiques:</p>
<ul>
<li><span class="math inline">\(^{0}\!D\)</span> est le nombre d’espèces;</li>
<li><span class="math inline">\(^{1}\!D=e^H\)</span>, l’exponentielle de l’indice de Shannon;</li>
<li><span class="math inline">\(^{2}\!D={1}/{\left(1-E\right)}\)</span>, l’inverse de l’indice de concentration de Simpson, connu sous le nom d’indice de <span class="citation">Stoddart (<a href="#ref-Stoddart1983" role="doc-biblioref">1983</a>)</span>.</li>
</ul>
<p>Ces résultats avaient déjà été obtenus avec une autre approche par <span class="citation">MacArthur (<a href="#ref-MacArthur1965" role="doc-biblioref">1965</a>)</span> et repris par <span class="citation">Adelman (<a href="#ref-Adelman1969" role="doc-biblioref">1969</a>)</span> dans la littérature économique.</p>
<p>Les nombres de Hill sont des “nombres effectifs” ou “nombres équivalents”.
Le concept a été défini rigoureusement par <span class="citation">Gregorius (<a href="#ref-Gregorius1991" role="doc-biblioref">1991</a>)</span>, d’après <span class="citation">Wright (<a href="#ref-Wright1931" role="doc-biblioref">1931</a>)</span> (qui avait le premier défini la taille effective d’une population): étant donné une variable caractéristique (ici, l’entropie) fonction seulement d’une variable numérique (ici, le nombre d’espèces) dans un cas idéal (ici, l’équiprobabilité des espèces), le nombre effectif est la valeur de la variable numérique pour laquelle la variable caractéristique est celle du jeu de données.</p>
<p><span class="citation">Gregorius (<a href="#ref-Gregorius2014" role="doc-biblioref">2014</a>)</span> montre que de nombreux autres indices de diversité sont acceptables dans le sens où ils vérifient les axiomes précédents et, de plus, que la diversité d’un assemblage de communautés est obligatoirement supérieure à la diversité moyenne de ces communautés (l’égalité n’étant possible que si les communautés sont toutes identiques).
Cette dernière propriété sera traitée en détail dans la partie consacrée à la décomposition de la diversité.
Ces indices doivent vérifier deux propriétés: leur fonction d’information doit être décroissante, et ils doivent être une fonction strictement concave de <span class="math inline">\(p_s\)</span>.
Parmi les possibilités, <span class="math inline">\(I(p_s) = \cos{(p_s {\pi}/{2})}\)</span> est envisageable par exemple: le choix de la fonction d’information est virtuellement illimité, mais seules quelques unes seront interprétables clairement.</p>
<p>Un nombre équivalent d’espèces existe pour tous ces indices, il est toujours égal à l’inverse de l’image de l’indice par la réciproque de la fonction d’information:</p>
<p><span class="math display" id="eq:Gregorius2014">\[\begin{equation}
  \tag{4.4}
  D = \frac{1}{I^{-1}\left(\sum^S_{s=1}{p_s I(p_s)}\right)}.
\end{equation}\]</span></p>
<p>D’autres entropies ont été utlisées, avec plus ou moins de succès.
Par exemple, <span class="citation">Ricotta et Avena (<a href="#ref-Ricotta2003c" role="doc-biblioref">2003</a><a href="#ref-Ricotta2003c" role="doc-biblioref">b</a>)</span> proposent d’utiliser la fonction d’information <span class="math inline">\(I(p_s)=-\ln(k_s)\)</span> où <span class="math inline">\(k_s\)</span> est la dissimilarité totale de l’espèce <span class="math inline">\(s\)</span> avec les autres (par exemple, la somme des distances aux autres espèces dans un arbre phylogénétique, voir section <a href="chap-cadrephyfonc.html#sec-Dphylo">6.2</a>), normalisée pour que <span class="math inline">\(\sum_s{k_s}=1\)</span>.
Ainsi, les espèces les plus originales apportent peu d’information, ce qui n’est pas très intuitif.
Les auteurs montrent que leur mesure est la somme de l’entropie de Shannon et de la divergence de Kullback-Leibler entre les probabilités et les dissimilarités des espèces.
<span class="citation">Ricotta et Szeidl (<a href="#ref-Ricotta2006b" role="doc-biblioref">2006</a>)</span> ont défini plus tard une entropie augmentant avec l’originalité de chaque espèce, présentée au chapitre <a href="chap-LeinsterCobbold.html#chap-LeinsterCobbold">9</a>.</p>
</div>
<div id="entropie-hcdt" class="section level2">
<h2><span class="header-section-number">4.4</span> Entropie HCDT</h2>
<p><span class="citation">Tsallis (<a href="#ref-Tsallis1988" role="doc-biblioref">1988</a>)</span> propose une classe de mesures appelée entropie généralisée, définie par <span class="citation">Havrda et Charvát (<a href="#ref-Havrda1967" role="doc-biblioref">1967</a>)</span> pour la première fois et redécouverte plusieurs fois, notamment par <span class="citation">Daróczy (<a href="#ref-Daroczy1970" role="doc-biblioref">1970</a>)</span>, d’où son nom <em>entropie HCDT</em> (voir <span class="citation">Mendes et al. (<a href="#ref-Mendes2008" role="doc-biblioref">2008</a>)</span>, page 451, pour un historique complet):</p>
<p><span class="math display" id="eq:HCDT">\[\begin{equation}
  \tag{4.5}
  ^{q}\!H = \frac{1}{q-1}\left(1-\sum^S_{s=1}{p^q_s}\right).
\end{equation}\]</span></p>
<p>Tsallis a montré que les indices de Simpson et de Shannon étaient des cas particuliers d’entropie généralisée, retrouvant, sans faire le rapprochement <span class="citation">(Ricotta <a href="#ref-Ricotta2005" role="doc-biblioref">2005</a><a href="#ref-Ricotta2005" role="doc-biblioref">c</a>)</span>, la définition d’un indice de diversité de <span class="citation">Patil et Taillie (<a href="#ref-Patil1982" role="doc-biblioref">1982</a>)</span>.</p>
<p>Ces résultats ont été complétés par d’autres et repris en écologie par <span class="citation">Keylock (<a href="#ref-Keylock2005" role="doc-biblioref">2005</a>)</span> et Jost <span class="citation">(<a href="#ref-Jost2006" role="doc-biblioref">2006</a>, <a href="#ref-Jost2007" role="doc-biblioref">2007</a>)</span>.
Là encore:</p>
<ul>
<li>Le nombre d’espèces moins 1 est <span class="math inline">\(^{0}\!H\)</span>;</li>
<li>L’indice de Shannon est <span class="math inline">\(^{1}\!H\)</span>;</li>
<li>L’indice de Gini-Simpson est <span class="math inline">\(^{2}\!H\)</span>.</li>
</ul>
<p>L’entropie HCDT est particulièrement attractive parce que sa relation avec la diversité au sens strict est simple, après introduction du formalisme adapté (les logarithmes déformés).
Son biais d’estimation peut être corrigé globalement, et non seulement pour les cas particuliers (nombre d’espèces, Shannon, Simpson).
Enfin, sa décomposition sera présentée en détail dans le chapitre <a href="chap-DedompHCDT.html#chap-DedompHCDT">12</a>.</p>
<div id="logarithmes-déformés" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Logarithmes déformés</h3>
<p>L’écriture de l’entropie HCDT est largement simplifiée en introduisant le formalisme des logarithmes déformés <span class="citation">(Tsallis <a href="#ref-Tsallis1994" role="doc-biblioref">1994</a>)</span>.
Le logarithme d’ordre <span class="math inline">\(q\)</span> est défini par
<span class="math display" id="eq:lnq">\[\begin{equation}
  \tag{4.6}
  \ln_q{x} = \frac{x^{1-q}-1}{1-q},
\end{equation}\]</span>
dont la forme est identique à la transformation de <span class="citation">Box et Cox (<a href="#ref-Box1964" role="doc-biblioref">1964</a>)</span> utilisée en statistiques pour normaliser une variable.</p>
<p>Le logarithme déformé converge vers le logarithme naturel quand <span class="math inline">\(q\to 1\)</span> (figure <a href="entropie.html#fig:lnqFig">4.4</a>).</p>
<p>Le code R nécessaire pour réaliser la figure est:</p>
<p></p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="entropie.html#cb150-1"></a>ln0 &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="kw">lnq</span>(p, <span class="dv">0</span>)</span>
<span id="cb150-2"><a href="entropie.html#cb150-2"></a>ln2 &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="kw">lnq</span>(p, <span class="dv">2</span>)</span>
<span id="cb150-3"><a href="entropie.html#cb150-3"></a>lnm1 &lt;-<span class="st"> </span><span class="cf">function</span>(p) <span class="kw">lnq</span>(p, <span class="dv">-1</span>)</span>
<span id="cb150-4"><a href="entropie.html#cb150-4"></a><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb150-5"><a href="entropie.html#cb150-5"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> log) <span class="op">+</span></span>
<span id="cb150-6"><a href="entropie.html#cb150-6"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> ln0, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb150-7"><a href="entropie.html#cb150-7"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> ln2, <span class="dt">lty =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb150-8"><a href="entropie.html#cb150-8"></a><span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> lnm1, <span class="dt">lty =</span> <span class="dv">4</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>) <span class="op">+</span></span>
<span id="cb150-9"><a href="entropie.html#cb150-9"></a><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb150-10"><a href="entropie.html#cb150-10"></a><span class="st">    </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">y =</span> <span class="kw">expression</span>(ln[q](p)))</span></code></pre></div>
<p></p>
<p>Sa fonction inverse est l’exponentielle d’ordre <span class="math inline">\(q\)</span>:</p>
<p><span class="math display" id="eq:expq">\[\begin{equation}
  \tag{4.7}
  e^x_q = \left[ 1+\left( 1-q \right)x \right]^{\frac{1}{1-q}}.
\end{equation}\]</span></p>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lnqFig"></span>
<img src="MesuresBD_files/figure-html/lnqFig-1.png" alt="Valeur du logarithme d’ordre \(q\) de probabilités entre 0 et 1 pour différentes valeurs de \(q\): \(q = 0\) (pointillés longs rouges), la courbe est une droite; \(q = 1\) (trait plein): logarithme naturel; \(q = 2\) (pointillés courts bleus): la courbe a la même forme que le logarithme naturel pour les valeurs positives de \(q\); \(q =-1\) (pointillés alternés verts): la courbe est convexe pour les valeurs négatives de \(q\)." width="80%" />
<p class="caption">
Figure 4.4: Valeur du logarithme d’ordre <span class="math inline">\(q\)</span> de probabilités entre 0 et 1 pour différentes valeurs de <span class="math inline">\(q\)</span>: <span class="math inline">\(q = 0\)</span> (pointillés longs rouges), la courbe est une droite; <span class="math inline">\(q = 1\)</span> (trait plein): logarithme naturel; <span class="math inline">\(q = 2\)</span> (pointillés courts bleus): la courbe a la même forme que le logarithme naturel pour les valeurs positives de <span class="math inline">\(q\)</span>; <span class="math inline">\(q =-1\)</span> (pointillés alternés verts): la courbe est convexe pour les valeurs négatives de <span class="math inline">\(q\)</span>.
</p>
</div>
<p></p>
<p>Enfin, le logarithme déformé est subadditif:</p>
<p><span class="math display" id="eq:lnqsubadd">\[\begin{equation}
  \tag{4.8}
  \ln_q\left(xy\right)=\ln_q{x}+\ln_q{y}-\left(q-1\right)\left(\ln_q{x}\right)\left(\ln_q{y}\right).
\end{equation}\]</span></p>
<p>Ses propriétés sont les suivantes:</p>
<p><span class="math display" id="eq:lnqinv">\[\begin{equation}
  \tag{4.9}
  \ln_q\frac{1}{x}=-x^{q-1}\ln_qx;
\end{equation}\]</span></p>
<p><span class="math display" id="eq:lnqprod">\[\begin{equation}
  \tag{4.10}
  \ln_q\left(xy\right)=\ln_q{x}+x^{1-q}\ln_q{y};
\end{equation}\]</span></p>
<p><span class="math display" id="eq:lnqdiv">\[\begin{equation}
  \tag{4.11}
  \ln_q\left(\frac{x}{y}\right)=\ln_q{x}-{\left(\frac{x}{y}\right)}^{1-q}\ln_q{y};
\end{equation}\]</span></p>
<p>et</p>
<p><span class="math display" id="eq:expqsum">\[\begin{equation}
  \tag{4.12}
  e^{x+y}_q = e_q^x e^{\frac{y}{1+\left(1-q\right)x}}_q.
\end{equation}\]</span></p>
<p>Si <span class="math inline">\(q&gt;1\)</span>, <span class="math inline">\({\mathop{\lim}_{x\to +\infty} \left(\ln_q{x}\right)={1}/{\left(q-1\right)}}\)</span>, donc <span class="math inline">\(e^x_q\)</span> n’est pas définie pour <span class="math inline">\(x&gt;{1}/{\left(q-1\right)}\)</span>.</p>
<p>La dérivée du logarithme déformé est, quel que soit <span class="math inline">\(q\)</span>,
<span class="math display" id="eq:lnqprime">\[\begin{equation}
  \tag{4.13}
  \ln&#39;_q\left(x\right) = x^{-q}.
\end{equation}\]</span></p>
<p>Les dérivées première et seconde de l’exponentielle déformée sont, quel que soit <span class="math inline">\(q\)</span>:
<span class="math display" id="eq:expqprime">\[\begin{equation}
  \tag{4.14}
  \exp&#39;_q\left(x\right) = \left( e_q^x \right)^{q};
\end{equation}\]</span>
<span class="math display" id="eq:expqsec">\[\begin{equation}
  \tag{4.15}
  \exp&#39;&#39;_q\left(x\right) = \left( e_q^x \right)^{2q-1}.
\end{equation}\]</span></p>
<p>Ces fonctions sont implémentées dans le package <em>entropart</em>: <code>lnq(x, q)</code> et <code>expq(x, q)</code>.</p>
<p>L’entropie d’ordre <span class="math inline">\(q\)</span> s’écrit</p>
<p><span class="math display" id="eq:EntropieHCDT">\[\begin{equation}
  \tag{4.16}
  ^{q}\!H = \frac{1}{q-1}\left(1-\sum^S_{s=1}{p^q_s}\right)=-\sum_s{p^q_s}\ln_q{p_s}=\sum_s{p_s}\ln_q\frac{1}{p_s}.
\end{equation}\]</span></p>
<p>Ces trois formes sont équivalentes mais les deux dernières s’interprètent comme une généralisation de l’entropie de Shannon <span class="citation">(Marcon et al. <a href="#ref-Marcon2014a" role="doc-biblioref">2014</a>)</span>.</p>
<p>Le calcul de <span class="math inline">\(^{q}\!H\)</span> peut se faire avec la fonction <code>Tsallis</code> de la librairie <em>entropart</em>:</p>
<p></p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="entropie.html#cb151-1"></a>Ps &lt;-<span class="st"> </span><span class="kw">as.ProbaVector</span>(<span class="kw">colSums</span>(BCI))</span>
<span id="cb151-2"><a href="entropie.html#cb151-2"></a>q &lt;-<span class="st"> </span><span class="fl">1.5</span></span>
<span id="cb151-3"><a href="entropie.html#cb151-3"></a><span class="kw">Tsallis</span>(Ps, q)</span></code></pre></div>
<pre><code>##     None 
## 1.715954</code></pre>
<p></p>
</div>
<div id="entropie-et-diversité" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Entropie et diversité</h3>
<p>On voit immédiatement que l’entropie de Tsallis est le logarithme d’ordre <span class="math inline">\(q\)</span> du nombre de Hill correspondant, comme l’entropie de Rényi en est le logarithme naturel:</p>
<p><span class="math display" id="eq:HlnD">\[\begin{equation}
  \tag{4.17}
  ^{q}\!H = \ln_q{^{q}\!D};
\end{equation}\]</span></p>
<p><span class="math display" id="eq:DexpH">\[\begin{equation}
  \tag{4.18}
  ^{q}\!D = e_q^{^{q}\!H}.
\end{equation}\]</span></p>
<p>L’entropie est utile pour les calculs: la correction des biais d’estimation notamment.
Les nombres de Hill, ou <em>nombres équivalents d’espèces</em> ou <em>nombres effectif d’espèces</em> permettent une appréhension plus intuitive de la notion de biodiversité <span class="citation">(Jost <a href="#ref-Jost2006" role="doc-biblioref">2006</a>)</span>.
En raison de leurs propriétés, notamment de décomposition (voir le chapitre <a href="chap-DedompHCDT.html#chap-DedompHCDT">12</a>), <span class="citation">Jost (<a href="#ref-Jost2007" role="doc-biblioref">2007</a>)</span> les appelle “vraie diversité”.
<span class="citation">Hoffmann et Hoffmann (<a href="#ref-Hoffmann2008" role="doc-biblioref">2008</a>)</span> critiquent cette définition totalitaire et fournissent une revue historique plus lointaine sur les origines de ces mesures.
<span class="citation">Jost (<a href="#ref-Jost2009" role="doc-biblioref">2009</a><a href="#ref-Jost2009" role="doc-biblioref">b</a>)</span> reconnaît qu’un autre terme aurait pu être choisi (“diversité neutre” ou “diversité mathématique” par exemple).</p>
<p><span class="citation">Dauby et Hardy (<a href="#ref-Dauby2012" role="doc-biblioref">2012</a>)</span> écrivent “diversité au sens strict”; <span class="citation">Gregorius (<a href="#ref-Gregorius2010" role="doc-biblioref">2010</a>)</span> “diversité explicite”.</p>
<p>Quoi qu’il en soit, les nombres de Hill respectent le principe de réplication (voir <span class="citation">Chao, Chiu, et Jost (<a href="#ref-Chao2010" role="doc-biblioref">2010</a>)</span>, section 3 pour une discussion et un historique): si <span class="math inline">\(I\)</span> communautés de même taille, de même niveau de diversité <span class="math inline">\(D\)</span>, mais sans espèces en commun sont regroupées dans une méta-communauté, la diversité de la méta-communauté doit être <span class="math inline">\(I\times D\)</span>.</p>
<p>L’intérêt de ces approches est de fournir une définition paramétrique de la diversité, qui donne plus ou moins d’importance aux espèces rares:</p>
<ul>
<li><span class="math inline">\(^{-\infty}\!D={1}/{\min(p_S)}\)</span> est l’inverse de la proportion de la communauté représentée par l’espèce la plus rare (toutes les autres espèces sont ignorées).
Le biais d’estimation est incontrôlable: l’espèce la plus rare n’est pas dans l’échantillon tant que l’inventaire n’est pas exhaustif;</li>
<li><span class="math inline">\(^{0}\!D\)</span> est le nombre d’espèces (alors que <span class="math inline">\(^{0}\!H\)</span> est le nombre d’espèces moins 1). C’est la mesure classique qui donne le plus d’importance aux espèces rares: toutes les espèces ont la même importance, quel que soit leur effectif en termes d’individus.
Il est bien adapté à une approche patrimoniale, celle du collectionneur qui considère que l’existence d’une espèce supplémentaire a un intérêt en soi, par exemple parce qu’elle peut contenir une molécule valorisable.
Comme les espèces rares sont difficiles à échantillonner, le biais d’estimation est très important, et sa résolution a généré une littérature en soi;</li>
<li><span class="math inline">\(^{1}\!D\)</span> est l’exponentielle de l’indice de Shannon donne la même importance à tous les individus.
Il est adapté à une approche d’écologue, intéressé par les interactions possibles: le nombre de combinaisons d’espèces en est une approche satisfaisante.
Le biais d’estimation est sensible;</li>
<li><span class="math inline">\(^{2}\!D\)</span> est l’inverse de l’indice de concentration de Gini-Simpson donne moins d’importance aux espèces rares.
<span class="citation">Hill (<a href="#ref-Hill1973" role="doc-biblioref">1973</a>)</span> l’appelle “le nombre d’espèces très abondantes”.
Il comptabilise les interactions possibles entre paires d’individus: les espèces rares interviennent dans peu de paires, et influent peu sur l’indice.
En conséquence, le biais d’estimation est très petit; de plus, un estimateur non biaisé existe;</li>
<li><span class="math inline">\(^{\infty}\!D={1}/{d}\)</span> est l’inverse de l’indice de Berger-Parker <span class="citation">(Berger et Parker <a href="#ref-Berger1970" role="doc-biblioref">1970</a>)</span> qui est la proportion de la communauté représentée par l’espèce la plus abondante: <span class="math inline">\(d=\max(\mathbf{p})\)</span>.
Toutes les autres espèces sont ignorées.</li>
</ul>
<p>Le calcul de <span class="math inline">\(^{q}\!D\)</span> peut se faire avec la fonction <code>Diversity</code> de la librairie <em>entropart</em>:</p>
<p></p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="entropie.html#cb153-1"></a>Ps &lt;-<span class="st"> </span><span class="kw">as.ProbaVector</span>(<span class="kw">colSums</span>(BCI))</span>
<span id="cb153-2"><a href="entropie.html#cb153-2"></a>q &lt;-<span class="st"> </span><span class="fl">1.5</span></span>
<span id="cb153-3"><a href="entropie.html#cb153-3"></a><span class="kw">Diversity</span>(Ps, q)</span></code></pre></div>
<pre><code>##     None 
## 49.57724</code></pre>
<p></p>
<p>Les propriétés mathématiques de la diversité ne sont pas celles de l’entropie.
L’entropie doit être une fonction concave des probabilités comme on l’a vu plus haut, mais pas la diversité (un exemple de confusion est fourni par <span class="citation">Gadagkar (<a href="#ref-Gadagkar1989" role="doc-biblioref">1989</a>)</span>, qui reproche à <span class="math inline">\(^{2}\!D\)</span> de ne pas être concave).
L’entropie est une moyenne pondérée par les probabilités de la fonction d’information, c’est donc une fonction linéaire des probabilités, propriété importante pour définir l’entropie <span class="math inline">\(\alpha\)</span> (section <a href="sec-betaCadre.html#sec-defalpha">10.3.4</a>) comme la moyenne des entropies de plusieurs communautés, ou l’entropie phylogénétique (chapitre <a href="chap-Phyloentropie.html#chap-Phyloentropie">8</a>) comme la moyenne de l’entropie sur les périodes d’un arbre.
La diversité n’est pas une fonction linéaire des probabilités: la diversité moyenne n’est en général pas la moyenne des diversités.</p>
</div>
<div id="synthèse-1" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Synthèse</h3>
<p>L’inverse de la probabilité d’une espèce, <span class="math inline">\(1/p_s\)</span>, définit sa rareté.
L’entropie est la moyenne du logarithme de la rareté:</p>
<p><span class="math display" id="eq:EntropieRarete">\[\begin{equation}
  \tag{4.19}
  ^{q}\!H = \frac{1}{q-1}\left(1-\sum^S_{s=1}{p^q_s}\right)=\sum_s{p_s}\ln_q\frac{1}{p_s}.
\end{equation}\]</span></p>
<p>La diversité est son exponentielle:</p>
<p><span class="math display" id="eq:DexpHSynthese">\[\begin{equation}
  \tag{4.20}
  ^{q}\!D = e_q^{^{q}\!H}.
\end{equation}\]</span></p>
</div>
</div>
<div id="profils-de-diversité" class="section level2">
<h2><span class="header-section-number">4.5</span> Profils de diversité</h2>
<p><span class="citation">Leinster et Cobbold (<a href="#ref-Leinster2012" role="doc-biblioref">2012</a>)</span>, après <span class="citation">Hill (<a href="#ref-Hill1973" role="doc-biblioref">1973</a>)</span>, <span class="citation">Patil et Taillie (<a href="#ref-Patil1982" role="doc-biblioref">1982</a>)</span>, <span class="citation">Tothmeresz (<a href="#ref-Tothmeresz1995" role="doc-biblioref">1995</a>)</span> et <span class="citation">Kindt, Van Damme, et Simons (<a href="#ref-Kindt2006" role="doc-biblioref">2006</a>)</span>, recommandent de tracer des profils de diversité, c’est-à-dire la valeur de la diversité <span class="math inline">\(^{q}\!D\)</span> en fonction de l’ordre <span class="math inline">\(q\)</span> (figure <a href="entropie.html#fig:ProfilDivFig">4.5</a>) pour comparer plusieurs communautés.
Une communauté peut être déclarée plus diverse qu’une autre si son profil de diversité est au-dessus de l’autre pour toutes les valeurs de <span class="math inline">\(q\)</span>.
Si les courbes se croisent, il n’y a pas de relation d’ordre <span class="citation">(Tothmeresz <a href="#ref-Tothmeresz1995" role="doc-biblioref">1995</a>)</span>.</p>
<p><span class="citation">Lande, DeVries, et Walla (<a href="#ref-Lande2000" role="doc-biblioref">2000</a>)</span> montrent que si la diversité de Simpson et la richesse de deux communautés n’ont pas le même ordre, alors les courbes d’accumulation du nombre d’espèces en fonction du nombre d’individus échantillonnés se croisent aussi.</p>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ProfilDivFig"></span>
<img src="MesuresBD_files/figure-html/ProfilDivFig-1.png" alt="Profil de diversité calculé pour deux parcelles de Paracou (Parcelle 6: trait plein et Parcelle 18: trait pointillé). La correction du biais d’estimation est celle de Chao et Jost." width="80%" />
<p class="caption">
Figure 4.5: Profil de diversité calculé pour deux parcelles de Paracou (Parcelle 6: trait plein et Parcelle 18: trait pointillé). La correction du biais d’estimation est celle de Chao et Jost.
</p>
</div>
<p></p>
<p>Code R pour réaliser la figure <a href="entropie.html#fig:ProfilDivFig">4.5</a>:</p>
<p></p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="entropie.html#cb155-1"></a>q.seq &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="fl">.1</span>)</span>
<span id="cb155-2"><a href="entropie.html#cb155-2"></a>  P6D&lt;-<span class="st"> </span><span class="kw">CommunityProfile</span>(Diversity, Paracou618.MC<span class="op">$</span>Nsi[, <span class="dv">1</span>], q.seq)</span>
<span id="cb155-3"><a href="entropie.html#cb155-3"></a>  P18D&lt;-<span class="st"> </span><span class="kw">CommunityProfile</span>(Diversity, Paracou618.MC<span class="op">$</span>Nsi[, <span class="dv">2</span>], q.seq)</span>
<span id="cb155-4"><a href="entropie.html#cb155-4"></a>  <span class="kw">autoplot</span>(P6D, <span class="dt">xlab =</span> <span class="st">&quot;q&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Diversité&quot;</span>) <span class="op">+</span></span>
<span id="cb155-5"><a href="entropie.html#cb155-5"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(x, y), <span class="kw">as.data.frame.list</span>(P18D), <span class="dt">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p><span class="citation">Pallmann et al. (<a href="#ref-Pallmann2012" role="doc-biblioref">2012</a>)</span> ont développé un test statistique pour comparer la diversité de deux communautés pour plusieurs valeurs de <span class="math inline">\(q\)</span> simultanément.</p>
<p><span class="citation">Liu et al. (<a href="#ref-Liu2006" role="doc-biblioref">2006</a>)</span> nomment <em>séparables</em> des communautés dont les profils ne se croisent pas.
Ils montrent que des communautés peuvent être séparables en selon un profil de diversité de <span class="math inline">\(^{q}\!D\)</span> sans l’être forcément selon un profil de diversité de Hurlbert (section <a href="chap-MesuresNeutres.html#sec-Hurlbert">3.4</a>), et inversement.
Ils montrent que les communautés séparables selon un troisième type de profil, celui de la queue de distribution <span class="citation">(Patil et Taillie <a href="#ref-Patil1982" role="doc-biblioref">1982</a>)</span>, le sont dans tous les cas.
Le profil de la queue de distribution est construit en classant les espèces de la plus fréquente à la plus rare et en traçant la probabilité qu’un individu appartienne à une espèce plus rare que l’espèce en abscisse (figure <a href="entropie.html#fig:ProfilQueueFig">4.6</a>).</p>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ProfilQueueFig"></span>
<img src="MesuresBD_files/figure-html/ProfilQueueFig-1.png" alt="Profil de queue de distribution calculé pour les deux parcelles de Paracou (Parcelle 6: trait plein et Parcelle 18: trait pointillé). En abscisse: rang de l’espèce dans le classement de la plus fréquente à la plus rare; en ordonnée: probabilité qu’un individu de la communauté appartienne à une espèce plus rare." width="80%" />
<p class="caption">
Figure 4.6: Profil de queue de distribution calculé pour les deux parcelles de Paracou (Parcelle 6: trait plein et Parcelle 18: trait pointillé). En abscisse: rang de l’espèce dans le classement de la plus fréquente à la plus rare; en ordonnée: probabilité qu’un individu de la communauté appartienne à une espèce plus rare.
</p>
</div>
<p></p>
<p>Code R:</p>
<p></p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="entropie.html#cb156-1"></a><span class="co"># Elimination des espèces absentes des deux parcelles</span></span>
<span id="cb156-2"><a href="entropie.html#cb156-2"></a>Psi &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(Paracou618.MC<span class="op">$</span>Psi[Paracou618.MC<span class="op">$</span>Ps <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>,</span>
<span id="cb156-3"><a href="entropie.html#cb156-3"></a>    ])</span>
<span id="cb156-4"><a href="entropie.html#cb156-4"></a>Psi[, <span class="dv">1</span>] <span class="op">%&lt;&gt;%</span></span>
<span id="cb156-5"><a href="entropie.html#cb156-5"></a><span class="st">    </span>sort <span class="op">%&gt;%</span></span>
<span id="cb156-6"><a href="entropie.html#cb156-6"></a><span class="st">    </span>cumsum <span class="op">%&gt;%</span></span>
<span id="cb156-7"><a href="entropie.html#cb156-7"></a><span class="st">    </span>rev</span>
<span id="cb156-8"><a href="entropie.html#cb156-8"></a>Psi[, <span class="dv">2</span>] <span class="op">%&lt;&gt;%</span></span>
<span id="cb156-9"><a href="entropie.html#cb156-9"></a><span class="st">    </span>sort <span class="op">%&gt;%</span></span>
<span id="cb156-10"><a href="entropie.html#cb156-10"></a><span class="st">    </span>cumsum <span class="op">%&gt;%</span></span>
<span id="cb156-11"><a href="entropie.html#cb156-11"></a><span class="st">    </span>rev</span>
<span id="cb156-12"><a href="entropie.html#cb156-12"></a>Psi<span class="op">$</span>x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Psi)</span>
<span id="cb156-13"><a href="entropie.html#cb156-13"></a><span class="kw">ggplot</span>(Psi, <span class="kw">aes</span>(x)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_path</span>(<span class="kw">aes</span>(<span class="dt">y =</span> P006)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_path</span>(<span class="kw">aes</span>(<span class="dt">y =</span> P018),</span>
<span id="cb156-14"><a href="entropie.html#cb156-14"></a>    <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Espèces&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Fréquence&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">scale_x_log10</span>()</span></code></pre></div>
<p></p>
<p>Les coordonnées des points du profil sont définies par
<span class="math display" id="eq:ProfilQueue">\[\begin{equation}
  \tag{4.21}
  y(x) = \sum_{s=x+1}^{S}{p_{[s]}},\ x\in \{0, 1, \dots, S\}.
\end{equation}\]</span></p>
<p><span class="math inline">\(p_{[s]}\)</span> est la probabilité de l’espèce <span class="math inline">\(s\)</span>; les espèces sont classées par probabilité décroissante.</p>
<p>Ce profil est exhaustif (toutes les espèces sont représentées) alors que les autres profils de diversité ne sont représentés que pour un intervalle restreint du paramètre et qu’un croisement de courbes peut se produire au-delà.
En revanche, il ne prend pas en compte les espèces non observées.</p>
<p><span class="citation">Fattorini et Marcheselli (<a href="#ref-Fattorini1999" role="doc-biblioref">1999</a>)</span> proposent un test pour comparer deux profils de queue de distribution à partir d’échantillonnages multiples (nécessaires pour évaluer la variance de chacune des probabilités) mais qui néglige les espèces non observées.</p>
</div>
<div id="estimation-de-lentropie" class="section level2">
<h2><span class="header-section-number">4.6</span> Estimation de l’entropie</h2>
<div id="zhang-et-grabchak" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Zhang et Grabchak</h3>
<p><span class="citation">Zhang et Grabchak (<a href="#ref-Zhang2014" role="doc-biblioref">2016</a>)</span> montrent que toutes les mesures d’entropie usuelles sont des combinaisons linéaires de <span class="math inline">\({\zeta}_{u,v}\)</span> <a href="chap-MesuresNeutres.html#eq:zeta">(3.38)</a>.
À partir des estimateurs de <span class="math inline">\({\zeta}_{u,v}\)</span> <span class="citation">(Zhang et Zhou <a href="#ref-Zhang2010" role="doc-biblioref">2010</a>)</span>, l’estimateur suivant est proposé pour <span class="math inline">\(h_q=\sum^S_{s=1}{p^q_s}\)</span>:</p>
<p><span class="math display" id="eq:Zhang2014hq">\[\begin{equation}
  \tag{4.22}
  \tilde{h}_q = 1+\sum^{s^{n}_{\ne 0}}_{s=1}{\frac{n_s}{n}\sum^{n-n_s}_{v=1}{\left[\prod^v_{i=1}{\frac{i-q}{i}}\right]\left[\prod^v_{j=1}{\left(1-\frac{n_s-1}{n-j}\right)}\right]}}.
\end{equation}\]</span></p>
<p><span class="math inline">\(\tilde{h}_q\)</span> peut ensuite être utilisé pour l’estimation de l’entropie HCDT:
<span class="math display" id="eq:HqZhang">\[\begin{equation}
  \tag{4.23}
  ^q\!{\tilde{H}}
  = \frac{1-\tilde{h}_q}{q-1}.
\end{equation}\]</span></p>
<p>L’estimateur est asymptotiquement normal et son biais décroit exponentiellement vite.</p>
<p><span class="math inline">\(\tilde{h}_q\)</span> n’est défini que pour <span class="math inline">\(q\ne 1\)</span>.
L’estimateur corrigé de <span class="math inline">\(^{1}\!H\)</span> est</p>
<p><span class="math display" id="eq:H1Zhang">\[\begin{equation}
  \tag{4.24}
  ^1\!{\tilde{H}} =\sum^{s^{n}_{\ne 0}}_{s=1}{\frac{n_s}{n}\sum^{n-n_s}_{v=1}{\frac{1}{v}\left[\prod^v_{j=1}{1-\frac{n_s-1}{n-j}}\right]}}.
\end{equation}\]</span></p>
<p>Cet estimateur est différent de <span class="math inline">\(H_z\)</span> <span class="citation">(Zhang <a href="#ref-Zhang2012" role="doc-biblioref">2012</a>)</span>.
Il a été également obtenu par d’autres auteurs <span class="citation">(Mao <a href="#ref-Mao2007" role="doc-biblioref">2007</a>; Vinck et al. <a href="#ref-Vinck2012" role="doc-biblioref">2012</a>; Chao, Wang, et Jost <a href="#ref-Chao2013" role="doc-biblioref">2013</a>, Annexe S1)</span> avec d’autres approches.</p>
<p>Dans <em>entropart</em>, utiliser <code>Tsallis</code> et <code>Diversity</code>:</p>
<p></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="entropie.html#cb157-1"></a><span class="kw">Tsallis</span>(Paracou618.MC<span class="op">$</span>Ns, <span class="dt">q =</span> <span class="dv">1</span>, <span class="dt">Correction =</span> <span class="st">&quot;ZhangGrabchak&quot;</span>)</span></code></pre></div>
<pre><code>## ZhangGrabchak 
##      4.846407</code></pre>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="entropie.html#cb159-1"></a><span class="kw">Diversity</span>(Paracou618.MC<span class="op">$</span>Ns, <span class="dt">q =</span> <span class="dv">1</span>, <span class="dt">Correction =</span> <span class="st">&quot;ZhangGrabchak&quot;</span>)</span></code></pre></div>
<pre><code>## ZhangGrabchak 
##      127.2823</code></pre>
<p></p>
</div>
<div id="sec-BiaisHCDT" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Chao et Jost</h3>
<p><span class="citation">Chao et Jost (<a href="#ref-Chao2015" role="doc-biblioref">2015</a>)</span> complètent cet estimateur par une estimation du biais résiduel, selon la même démarche que pour l’entropie de Shannon <span class="citation">(Chao, Wang, et Jost <a href="#ref-Chao2013" role="doc-biblioref">2013</a>)</span>.
<span class="math inline">\(h_q\)</span> s’écrit sous la forme de la somme infinie des entropies de Simpson généralisées: <span class="math inline">\(h_q = \sum_{k=0}^{\infty}{\binom{q-1}{k}(-1)^k\zeta_{1,k}}\)</span>.
Alors que l’estimateur de Zhang et Grabchak ne prend en compte que les <span class="math inline">\(n-1\)</span> premiers termes de la somme pour un estimateur sans biais; <span class="citation">Chao et Jost (<a href="#ref-Chao2015" role="doc-biblioref">2015</a>)</span>, eqn 5, le complètent par une estimation, forcément biaisée, des termes suivants pour obtenir:</p>
<p><span class="math display" id="eq:HqChaoJost">\[\begin{align}
  \tag{4.25}
  ^q\!{\tilde{H}} = 
  &amp;\frac{1}{q-1} \\
  &amp;\left[ 1 -\tilde{h}_q -\frac{s_{1}}{n} {\left( 1-A \right)}^{1-n} \left( A^{q-1} -\sum^{n-1}_{r=0}{ \binom{q-1}{r} {\left(A-1\right)}^r} \right) \right].
\end{align}\]</span></p>
<p>La formule du nombre de combinaisons est généralisée aux valeurs de <span class="math inline">\(q-1\)</span> non entières, et vaut par convention 1 pour <span class="math inline">\(r=0\)</span>. <span class="math inline">\(A\)</span> vaut:</p>
<ul>
<li><span class="math inline">\(2s_{2}/{\left[\left(n-1\right) s_{1} +2s_{2}\right]}\)</span> en présence de singletons et doubletons;</li>
<li><span class="math inline">\(2/{\left[\left(n-1\right)\left(s_{1} -1\right)+2\right]}\)</span> en présence de singletons seulement;</li>
<li><span class="math inline">\(1\)</span> en absence de singletons et doubletons.</li>
</ul>
<p>Cet estimateur est le plus performant. Quand <span class="math inline">\(q \to 1\)</span>, il tend vers l’estimateur de l’entropie de Shannon de <span class="citation">Chao, Wang, et Jost (<a href="#ref-Chao2013" role="doc-biblioref">2013</a>)</span>, équation <a href="chap-MesuresNeutres.html#eq:Chao2013">(3.41)</a>.
Quand <span class="math inline">\(q=0\)</span>, l’estimateur se réduit à Chao1. Pour toutes les valeurs entières de <span class="math inline">\(q\)</span> supérieures ou égales à 2, il se réduit à l’estimateur <a href="entropie.html#eq:HqZhang">(4.23)</a>, identique à l’estimateur de Lande pour <span class="math inline">\(q=2\)</span>, et est sans biais.</p>
<p>L’estimateur est obtenu à partir de celui du taux de couverture extrapolé (section <a href="chap-Outils.html#sec-Couverture">2.3</a>) fourni par <span class="citation">Chao et Jost (<a href="#ref-Chao2012b" role="doc-biblioref">2012</a>)</span>, annexe E, sans le détail de la démonstration présenté ici.
L’espérance conditionnelle du taux de couverture dans un échantillon de taille <span class="math inline">\(n+m\)</span> compte-tenu des effectifs observés, notés <span class="math inline">\(\{n_s\}\)</span>, est</p>
<p><span class="math display" id="eq:EspCnm">\[\begin{equation}
  \tag{4.26}
  {\mathbb E}\left(C^{n+m} | \{n_s\} \right)
  = C^{n} + \sum_s{p_s [1-(1-p_s)^m] \mathbf{1}(n_s=0)}.
\end{equation}\]</span></p>
<p>Le taux de couverture <span class="math inline">\(C^{n}\)</span> est augmenté par l’échantillonnage de <span class="math inline">\(m\)</span> individus supplémentaires si les espèces non échantillonnées (vérifiant <span class="math inline">\(\mathbf{1}(n_s=0)\)</span>) sont découvertes (la probabilité de le faire est <span class="math inline">\(1-(1-p_s)^m\)</span>).</p>
<p><span class="math inline">\(\hat{C}^{n} + \sum_s{p_s \mathbf{1}(n_s=0)}\)</span> vaut par définition 1.</p>
<p>Le deuxième terme de la somme est estimé en posant <span class="math inline">\(\hat{p}_s = \hat{\alpha}_0\)</span> pour toutes les espèces non observées, ce qui constitue la même approximation que pour l’estimation de la relation de Good-Turing, équation <a href="chap-Outils.html#eq:GoodTuring2014">(2.2)</a>.
Alors, <span class="math inline">\((1-p_s)^m\)</span> est estimé par <span class="math inline">\((1-\hat{\alpha}_0)^m\)</span> et peut sortir de la somme qui ne contient plus que le terme <span class="math inline">\(\sum_s{p_s}\mathbf{1}(n_s=0)\)</span>, estimé par <span class="math inline">\(\hat{\alpha}_0 {s^{n}_{0}} = \frac{s^{n}_{1}}{n}(1 - \hat{\alpha}_1)\)</span>.</p>
<p>Les deux estimateurs <span class="math inline">\(\hat{\alpha}_1\)</span> et <span class="math inline">\(\hat{\alpha}_0\)</span> sont égaux à <span class="math inline">\(A\)</span> si le nombre d’espèces non échantillonnées est estimé par l’estimateur Chao1.
On obtient</p>
<p><span class="math display" id="eq:Cnm">\[\begin{align} 
  \tag{4.27}
  {\mathbb E}\left(C^{n+m} \right)
  &amp;= 1-\frac{s^{n}_{1}}{n} 
    \left[ \frac{\left(n-1\right)s^{n}_{1}}{\left(n-1\right)s^{n}_{1}+2s^{n}_{2}} \right]^{n+m} \\
  &amp;= 1-\frac{s^{n}_{1}}{n} \left( 1-A \right)^{m+1}.
\end{align}\]</span></p>
<p>L’espérance du taux de couverture est directement liée à l’indice de Simpson généralisé <span class="citation">(Good <a href="#ref-Good1953" role="doc-biblioref">1953</a>)</span>: <span class="math inline">\({\mathbb E}(C^{n+m}) = 1 - \zeta_{1,n+m}\)</span>.
L’estimateur de <span class="math inline">\(h_q\)</span> est obtenu en remplaçant les termes <span class="math inline">\(\zeta_{1,k}\)</span> de la somme infinie <span class="math inline">\(\sum_{k=n}^{\infty}{\binom{q-1}{k}(-1)^k \zeta_{1,k}}\)</span> par la valeur de <span class="math inline">\(1 -{\mathbb E}(C^{n+m})\)</span> calculée dans l’équation <a href="entropie.html#eq:Cnm">(4.27)</a>.
La formule obtenue contient le développement en série entière de <span class="math inline">\([1 - (1-A)^{q-1}]\)</span>, ce qui permet d’éliminer la somme infinie et d’obtenir l’estimateur de l’équation <a href="entropie.html#eq:HqChaoJost">(4.25)</a> après simplifications <span class="citation">(Chao et Jost <a href="#ref-Chao2015" role="doc-biblioref">2015</a>, Appendix S1, Theorem S1.2d)</span>.</p>
</div>
<div id="autres-méthodes" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Autres méthodes</h3>
<p>Deux autres approches sont possibles <span class="citation">(Marcon et al. <a href="#ref-Marcon2014a" role="doc-biblioref">2014</a>)</span>:</p>
<ul>
<li>Celle de <span class="citation">Chao et Shen (<a href="#ref-Chao2003" role="doc-biblioref">2003</a>)</span>, étendue au-delà de l’entropie de Shannon qui prend en compte les espèces non observées;</li>
<li>Celle de <span class="citation">Grassberger (<a href="#ref-Grassberger1988" role="doc-biblioref">1988</a>)</span> qui prend en compte la non-linéarité de l’estimateur.</li>
</ul>
<p>Le domaine de validité des deux corrections est différent.
Quand <span class="math inline">\(q\)</span> est petit, les espèces rares non observées ont une grande importance alors que <span class="math inline">\(^{q}\!H\)</span> est presque linéaire: la correction de Grassberger disparaît quand <span class="math inline">\(q=0\)</span>.
Quand <span class="math inline">\(q\)</span> est grand (au-delà de 2), l’influence des espèces rares est faible, la correction de Chao et Shen devient négligeable alors que la non-linéarité augmente.
Un compromis raisonnable consiste à utiliser le plus grand des deux estimateurs.</p>
<p>L’estimateur de Chao et Shen est</p>
<p><span class="math display" id="eq:HqChaoShen">\[\begin{equation}
  \tag{4.28}
  ^q\!{\tilde{H}} =-\sum^{s^{n}_{\ne 0}}_{s=1}{\frac{\hat{C}{\hat{p}}_s \ln_q\left(\hat{C}{\hat{p}}_s\right)}{1-{\left(1-\hat{C}{\hat{p}}_s\right)}^n}}.
\end{equation}\]</span></p>
<p>Celui de Grassberger est</p>
<p><span class="math display" id="eq:Grassberger">\[\begin{equation}
  \tag{4.29}
  ^q\!{\tilde{H}}
  = \frac{1-\sum^{s^{n}_{\ne 0}}_{s=1}{\widetilde{p^q_s}}}{q-1},
\end{equation}\]</span></p>
<p>où</p>
<p><span class="math display" id="eq:Estpqs">\[\begin{equation}
  \tag{4.30}
  \widetilde{p^q_s}
  = {n_s}^{-q} \left(
    \frac{\mathrm{\Gamma}\left(n_s +1\right)}{\mathrm{\Gamma}\left(n_s -q+1\right)}
    + \frac{{\left(-1\right)}^n \mathrm{\Gamma}\left(1+q\right)\sin{\pi q}}{\pi\left(n+1\right)}  \right).
\end{equation}\]</span></p>
<p><span class="math inline">\(\mathrm{\Gamma}\left(\cdot\right)\)</span> est la fonction gamma.
L’estimateur est linéaire par rapport à <span class="math inline">\(\widetilde{p^q_s}\)</span>: le problème original est donc résolu par la construction de l’estimateur de <span class="math inline">\(p^q_s\)</span>, mais celui-ci est biaisé également.</p>
<p>L’estimateur de Chao et Shen peut être amélioré <span class="citation">(Marcon <a href="#ref-Marcon2015a" role="doc-biblioref">2015</a>)</span> en estimant mieux les probabilités.
<span class="citation">Chao, Hsieh, et al. (<a href="#ref-Chao2014c" role="doc-biblioref">2015</a>)</span> ont développé un estimateur de la distribution des probabilités à partir de l’estimation du taux de couverture généralisé.
<span class="math inline">\(\hat{C}{\hat{p}}_s\)</span> peut être remplacé par cet estimateur pour réduire le biais de l’estimateur de Chao et Shen.</p>
<p>Enfin, la distribution des espèces non échantillonnées peut être ajoutée en estimant leur nombre et en choisissant une loi.
<span class="citation">Chao, Hsieh, et al. (<a href="#ref-Chao2014c" role="doc-biblioref">2015</a>)</span> ont utilisé l’estimateur Chao1 et une distribution géométrique.
L’estimateur jackknife <span class="citation">(Burnham et Overton <a href="#ref-Burnham1979" role="doc-biblioref">1979</a>)</span> est plus versatile parce que son ordre peut être adapté aux données quand l’effort d’échantillonnage est trop faible pour que l’estimateur Chao1 soit performant <span class="citation">(Brose, Martinez, et Williams <a href="#ref-Brose2003" role="doc-biblioref">2003</a>)</span>.
Un simple estimateur plug-in, appelé “estimateur révélé”, peut ensuite être appliqué à cette distribution révélée <span class="citation">(Marcon <a href="#ref-Marcon2015a" role="doc-biblioref">2015</a>)</span>.</p>
<p>L’estimateur du taux de couverture généralisé ne fait aucune hypothèse sur les espèces non observées, et n’utilise que le taux de couverture et la technique de Horvitz-Thompson pour estimer leur contribution.
L’estimateur révélé ne tente aucune correction mais utilise l’estimation la meilleure possible pour la distribution des probabilités.</p>
</div>
<div id="biais-des-estimateurs" class="section level3">
<h3><span class="header-section-number">4.6.4</span> Biais des estimateurs</h3>
<p>Étant donné la distribution des probabilités des espèces dans la communauté, l’échantillon
inventorié peut être considéré comme un tirage dans une loi multinomiale, qui contient pas toutes les espèces.
Par simulation à partir d’une distribution connue, il est possible de tirer un grand nombre d’échantillons et de répéter l’estimation de la diversité sur chacun d’eux.
La distribution de la valeur de l’échantillon permet de juger ses performances: son biais moyen (mesuré par rapport à la valeur de référence calculée à partir des probabilités choisies), et sa variabilité (évaluée par sa variance empirique sur les simulations).
Un estimateur performant doit être peu biaisé mais aussi peu variable.
L’erreur quadratique moyenne, somme du carré du biais et de la variance, ou sa racine carrée dont la dimension est celle des données (<em>root mean square error</em>: RMSE), est un bon critère de test <span class="citation">(voir par exemple Chao, Wang, et Jost <a href="#ref-Chao2013" role="doc-biblioref">2013</a>, fig. 1)</span>.</p>
<p><span class="citation">Haegeman et al. (<a href="#ref-Haegeman2013" role="doc-biblioref">2013</a>)</span> proposent deux estimateurs de la diversité <span class="math inline">\(^{q}\!D\)</span> à partir des courbes de raréfaction et d’extrapolation du nombre d’espèces.
Les deux estimateurs bornent la vraie valeur de <span class="math inline">\(^{q}\!D\)</span> (leur variabilité n’est pas prise en compte, mais elle est négligeable dans le cadre utilisé).
L’estimateur minimal est très proche de celui de Chao et Jost, à la différence que les termes estimant le biais résiduel sont évalués par une approximation numérique.
L’estimateur maximal suppose que tous les individus non échantillonnés appartiennent à une espèce nouvelle, ce qui est effectivement le pire cas possible.
Les deux estimateurs sont appliqués à la diversité de communautés microbiennes (jusqu’à un million d’espèces distribuées selon une loi géométrique) et très sous-échantillonnées (un individu sur <span class="math inline">\(10^{10}\)</span> dans le pire des cas).</p>
<p>Les résultats présentent de très grands intervalles d’estimation (de plusieurs ordres de grandeur) entre la borne inférieure et la borne supérieure, ce qui montre que la méthode est inutilisable pour <span class="math inline">\(q&lt;1\)</span> approximativement.
L’estimateur minimal, qui correspond à l’état de l’art, sous-estime sévèrement la diversité quand l’échantillonnage est faible.
L’estimateur maximal est excessivement conservateur (il est conçu pour borner l’estimation dans la pire hypothèse).</p>
<p>Aux ordres de diversité supérieurs, les estimateurs convergent: même avec de petits échantillons, même en supposant que la richesse de la communauté est la plus grande possible, l’estimation de la diversité est précise dès <span class="math inline">\(q=1\)</span>.</p>
<p>Dans des conditions moins difficiles, correspondant à des communautés moins riches (typiquement des forêts tropicales), l’estimation est précise dès <span class="math inline">\(q=0,5\)</span> <span class="citation">(Marcon <a href="#ref-Marcon2015a" role="doc-biblioref">2015</a>)</span>.
Ces résultats sont développés plus loin.</p>
</div>
<div id="intervalle-de-confiance-des-estimateurs" class="section level3">
<h3><span class="header-section-number">4.6.5</span> Intervalle de confiance des estimateurs</h3>
<p>À partir de données d’inventaire, l’estimation du biais est impossible (sinon, le biais serait simplement ajouté à l’estimateur pour le rendre sans biais).
La variabilité de l’estimateur peut être évaluée en générant des communautés par bootstrap et en estimant leur diversité comme dans la méthode précédente.
Les communautés peuvent être simulées par tirage dans une loi multinomiale respectant les probabilités observées <span class="citation">(Marcon et al. <a href="#ref-Marcon2012a" role="doc-biblioref">2012</a>, <a href="#ref-Marcon2014a" role="doc-biblioref">2014</a>)</span>.
L’inconvénient est que les espèces dont les probabilités sont petites ne sont pas tirées, ce qui entraîne un nouveau biais d’estimation.
L’estimateur le corrige en partie, le biais d’estimation dû au non échantillonnage des espèces rares de la communauté (corrigé par l’estimateur appliqué aux données réelles) n’est pas accessible.
Il est donc nécessaire de recentrer les valeurs obtenues par simulation autour de la valeur obtenue sur les données.</p>
<p><span class="citation">Chao et Jost (<a href="#ref-Chao2015" role="doc-biblioref">2015</a>)</span> proposent une méthode plus élaborée qui consiste à reconstituer la distribution des probabilités de la communauté réelle le mieux possible avant d’en tirer les échantillons simulés.
L’estimateur naïf de la probabilité d’une espèce observée <span class="math inline">\(s\)</span> est <span class="math inline">\(\hat{p}_s = {n_s}/{n}\)</span>.
<span class="math inline">\(\hat{p}_s\)</span> est sans biais mais comme on ne dispose que d’une réalisation de la loi, il est préférable de chercher un estimateur sans biais conditionnellement aux espèces observées.
Il est obtenu par <span class="citation">Chao, Wang, et Jost (<a href="#ref-Chao2013" role="doc-biblioref">2013</a>)</span> à partir de l’espérance conditionnelle de <span class="math inline">\({N_s}/{n}\)</span>:</p>
<p><span class="math display" id="eq:Chao2013ENs">\[\begin{equation}
  \tag{4.31}
  {\mathbb E} \left( \frac{N_s}{n} | n_s&gt;0 \right)
  = \frac{p_s}{1-(1-p_s)^n}.
\end{equation}\]</span></p>
<p>L’estimateur de <span class="math inline">\(p_s\)</span> obtenu est</p>
<p><span class="math display" id="eq:Chao2013ps">\[\begin{equation}
  \tag{4.32}
  \tilde{p}_s
  = \frac{n_s}{n} \left[ {1-\hat{\lambda}\left(1-\frac{n_s}{n}\right)^n} \right],
\end{equation}\]</span></p>
<p>où</p>
<p><span class="math display" id="eq:Chao2013lambda">\[\begin{equation}
  \tag{4.33}
  \hat{\lambda}
  = \frac{1-\hat{C}}{\sum_{s=1}^{s^{n}_{\ne 0}} {\frac{n_s}{n} \left(1-\frac{n_s}{n}\right)^n}}.
\end{equation}\]</span></p>
<p><span class="math inline">\(\lambda\)</span> est un paramètre d’un modèle permettant d’estimer les valeurs de <span class="math inline">\(p_i\)</span> à partir de leur espérance conditionnelle et des données.
Le modèle complet est présenté par <span class="citation">Chao, Hsieh, et al. (<a href="#ref-Chao2014c" role="doc-biblioref">2015</a>)</span>.</p>
<p>Les probabilités des espèces non observées sont considérées comme égales par souci de simplification et parce qu’elles influencent peu la variance des estimateurs.
Leur somme est égale au déficit de couverture, et leur nombre estimé par Chao1.</p>
<p>L’intervalle de confiance de l’estimateur obtenu par cette technique à partir des données permet de connaître sa variabilité, mais pas son biais.
Il n’est absolument pas garanti (et peu probable si l’échantillonnage est faible et <span class="math inline">\(q\)</span> petit) que la vraie valeur de la diversité se trouve dans l’intervalle de confiance.</p>
</div>
<div id="pratique-de-lestimation" class="section level3">
<h3><span class="header-section-number">4.6.6</span> Pratique de l’estimation</h3>
<p>La conversion de l’estimateur de l’entropie en estimateur de la diversité pose un nouveau problème puisque <span class="math inline">\(^{q}\!D\)</span> n’est pas une transformation linéaire de <span class="math inline">\(^{q}\!H\)</span>.
Ce dernier biais peut être négligé <span class="citation">(Grassberger <a href="#ref-Grassberger1988" role="doc-biblioref">1988</a>)</span> parce que <span class="math inline">\(^{q}\!H\)</span>, l’entropie, est la valeur moyenne de l’information de nombreuses espèces indépendantes et fluctue donc peu à cause de l’échantillonnage, contrairement aux estimateurs des probabilités.</p>
<p>Des simulations menées sur des distributions log-normale ou géométrique de richesse diverses avec des efforts d’échantillonnage variés permettent de tester les estimateurs dans des cas théoriques réalistes <span class="citation">(Marcon <a href="#ref-Marcon2015a" role="doc-biblioref">2015</a>)</span>.
Un résultat représentatif est donné ici.</p>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Estimation"></span>
<img src="images/Estimation.png" alt="Estimation de la diversité d’une communauté log-normale de 300 espèces. Un inventaire de 500 (en bas) ou 5000 (en haut) individus est répété 1000 fois et la diversité estimée à chaque fois par l’estimateur de Chao-Wang-Jost (à gauche) ou l’estimateur révélé (à droite). La courbe noire en gras représente la vraie diversité de la communauté. La courbe maigre est la moyenne de l’estimation, l’enveloppe grise limitée par les pointillés rouges contient 95% des valeurs simulées. A: 5000 individus, estimateur de Chao-Wang-Jost. B: 5000 individus, estimateur révélé. C: 500 individus, estimateur de Chao-Wang-Jost. D: 500 individus, estimateur révélé." width="100%" />
<p class="caption">
Figure 4.7: Estimation de la diversité d’une communauté log-normale de 300 espèces. Un inventaire de 500 (en bas) ou 5000 (en haut) individus est répété 1000 fois et la diversité estimée à chaque fois par l’estimateur de Chao-Wang-Jost (à gauche) ou l’estimateur révélé (à droite). La courbe noire en gras représente la vraie diversité de la communauté. La courbe maigre est la moyenne de l’estimation, l’enveloppe grise limitée par les pointillés rouges contient 95% des valeurs simulées. A: 5000 individus, estimateur de Chao-Wang-Jost. B: 5000 individus, estimateur révélé. C: 500 individus, estimateur de Chao-Wang-Jost. D: 500 individus, estimateur révélé.
</p>
</div>
<p></p>
<p>La figure <a href="entropie.html#fig:Estimation">4.7</a> compare les résultats des deux meilleurs estimateurs à la vraie valeur de la diversité d’une communauté de 300 espèces, dont la distribution log-normale (avec un écart-type dont le logarithme vaut 2) mime une forêt tropicale similaire à Barro-Colorado.
L’effort d’inventaire simulé est de 500 ou 5000 individus (de l’ordre d’un ou dix hectares pour les arbres de diamètre supérieur ou égal à 10 cm).
L’estimation ne pose pas de problème avec 5000 individus, bien que Chao-Wang-Jost sous-estime un peu le nombre d’espèces.
L’estimateur révélé est centré sur la vraie valeur, mais avec une plus grande variabilité.</p>
<p>En limitant l’échantillonnage à 500 individus, l’estimateur de Chao-Wang-Jost sous-estime le nombre d’espèces de moitié.
L’estimateur révélé sous-estime beaucoup moins la diversité d’ordre faible, mais son intervalle de confiance est très large.</p>
<p>Dès <span class="math inline">\(q=0,5\)</span>, l’estimation est très précise dans tous les cas, même en réduisant l’échantillonnage à 200 individus ou pour des communautés dont la queue de distribution est beaucoup plus longue (résultats non présentés ici).</p>
<p>Les conclusions de ce travail d’évaluation tiennent en deux points.
Dans des conditions réalistes d’inventaire d’arbres en forêt tropicale, les estimateurs de diversité les plus performants sont celui de Chao, Wang et Jost et l’estimateur révélé.
Le premier a une variance plus faible pour les valeurs de <span class="math inline">\(q\)</span> proches de 0 mais est limité par son estimation du nombre d’espèces par l’estimateur Chao1.
Le second est préférable quand le nombre d’espèces estimé par l’estimateur jackknife d’ordre optimal est clairement supérieur (c’est-à-dire, en pratique, quand l’ordre du jacknife optimal est supérieur à 1).
Son biais est alors bien inférieur, au prix d’une variance plus grande.</p>
<p>Dans tous les cas, l’estimation de la diversité aux ordres inférieurs à 0,5 est imprécise, sauf à disposer d’inventaires de taille considérable (de l’ordre de la dizaine d’hectares au moins).</p>
<p>Les fonctions <code>Tsallis</code> et <code>Diversity</code>de <em>entropart</em> acceptent comme argument un vecteur d’abondances (et non de probabilités) et proposent par défaut la correction de Chao et Jost:</p>
<p></p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="entropie.html#cb161-1"></a><span class="kw">Tsallis</span>(<span class="kw">colSums</span>(BCI), <span class="dt">q =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  UnveilJ 
## 4.276075</code></pre>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="entropie.html#cb163-1"></a><span class="kw">Diversity</span>(<span class="kw">colSums</span>(BCI), <span class="dt">q =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##  UnveilJ 
## 71.95748</code></pre>
<p></p>
</div>
<div id="msom-et-msam" class="section level3">
<h3><span class="header-section-number">4.6.7</span> MSOM et MSAM</h3>
<p>L’utilisation de modèles hiérarchiques bayesiens fondés sur des modèles d’occupation multispécifiques (MSOM) ou des modèles d’abondance multispécifiques (MSAM) progresse dans la littérature récente.
Ils nécessitent de choisir une loi de probabilité d’observation des espèces compatible avec un échantillonnage non exhaustif, par exemple une loi ZIP (<em>zero-inflated Poisson</em>) <span class="citation">(J. Zhang, Crist, et Hou <a href="#ref-Zhang2014c" role="doc-biblioref">2014</a>)</span>, dans laquelle l’espérance de la présence de chaque espèce peut être modélisée à son tour comme la conséquence de variables environnementales.
Une approche alternative consiste à modéliser la probabilité d’observation comme la combinaison d’une probabilité de présence et d’une probabilité de détection <span class="citation">(Broms, Hooten, et Fitzpatrick <a href="#ref-Broms2014" role="doc-biblioref">2014</a>)</span>.
Ces méthodes ont l’avantage de fournir une distribution de probabilité des paramètres des modèles, et donc de la diversité qui est calculée comme une quantité dérivée. Leur faiblesse est qu’ils dépendent de la loi choisie. Une revue complète est fournie par <span class="citation">Iknayan et al. (<a href="#ref-Iknayan2014" role="doc-biblioref">2014</a>)</span></p>
</div>
<div id="sec-RarExtrapol" class="section level3">
<h3><span class="header-section-number">4.6.8</span> Raréfaction et extrapolation</h3>
<p><span class="citation">Chao, Gotelli, et al. (<a href="#ref-Chao2014" role="doc-biblioref">2014</a>)</span> étendent l’approche de <span class="citation">Gotelli et Colwell (<a href="#ref-Gotelli2001" role="doc-biblioref">2001</a>)</span> aux nombres de Hill et fournissent les méthodes nécessaires au calcul de courbes de raréfaction et d’extrapolation de l’estimateur de <span class="math inline">\(^{q}\!D\)</span> en fonction de la taille de l’échantillon ou du taux de couverture.</p>
<p>Les courbes de raréfaction sont obtenues à partir de l’échantillon disponible en simulant la diminution de sa taille.
Les courbes d’extrapolation simulent l’augmentation de la taille de l’échantillon.
Elles sont en réalité calculées par raréfaction d’un échantillon hypothétique de taille infinie: un estimateur de <span class="math inline">\(^{q}\!D\)</span> corrigé du biais d’estimation est nécessaire.
L’article se limite aux valeurs de <span class="math inline">\(q\)</span> égales à 0, 1 ou 2 ou non entières mais supérieures à 2.
Le package <em>iNEXT</em> <span class="citation">(Hsieh, Ma, et Chao <a href="#ref-Hsieh2014" role="doc-biblioref">2016</a>)</span> implémente ces calculs dans R comme le package <em>entropart</em> qui améliore l’estimation des ordres non entiers de diversité.</p>
</div>
</div>
<div id="autres-approches" class="section level2">
<h2><span class="header-section-number">4.7</span> Autres approches</h2>
<div id="lentropie-de-rényi" class="section level3">
<h3><span class="header-section-number">4.7.1</span> L’entropie de Rényi</h3>
<p>L’entropie d’ordre <span class="math inline">\(q\)</span> de <span class="citation">Rényi (<a href="#ref-Renyi1961" role="doc-biblioref">1961</a>)</span> est</p>
<p><span class="math display" id="eq:Renyi">\[\begin{equation}
  \tag{4.34}
  ^{q}\!R =\frac{1}{1-q\ln\sum^S_{q=1}{p^q_s}}.
\end{equation}\]</span></p>
<p>C’est le logarithme naturel des nombres de <span class="citation">Hill (<a href="#ref-Hill1973" role="doc-biblioref">1973</a>)</span> qui ont été définis à partir d’elle.
L’entropie de Rényi a connu un succès important en tant que seule mesure d’entropie généralisée jusqu’à la diffusion de l’entropie HCDT.
Son défaut est qu’elle n’est pas forcément concave <span class="citation">(Beck <a href="#ref-Beck2009" role="doc-biblioref">2009</a>)</span>, ce qui empêche sa décomposition (voir chapitre <a href="chap-DedompHCDT.html#chap-DedompHCDT">12</a>).</p>
</div>
<div id="lentropie-généralisée-des-mesures-dinégalité" class="section level3">
<h3><span class="header-section-number">4.7.2</span> L’entropie généralisée des mesures d’inégalité</h3>
<p>L’entropie généralisée des économistes, d’ordre <span class="math inline">\(q\)</span>, est définie comme</p>
<p><span class="math display" id="eq:HqEco">\[\begin{equation}
  \tag{4.35}
  H_q = \frac{1}{q\left(q-1\right)} \left[\frac{1}{S}\sum^S_{s=1}{\left(\frac{n_s}{\bar{n_s}}\right)^{q}}-1\right].
\end{equation}\]</span></p>
<p>C’est en réalité une entropie relative, qui mesure l’écart entre la distribution observée <span class="math inline">\(\{n_s\}\)</span> et sa valeur moyenne. Elle tend vers l’entropie de Theil quand <span class="math inline">\(q \to 1\)</span> et la moyenne des <span class="math inline">\(\ln(\frac{n_s}{\bar{n_s}})\)</span> quand <span class="math inline">\(q \to 0\)</span>.
Elle a été conçue pour mesurer les inégalités <span class="citation">(<span class="citeproc-not-found" data-reference-id="Cowell2000"><strong>???</strong></span>)</span>, puis utilisée par exemple pour mesurer la concentration spatiale <span class="citation">(Brülhart et Traeger <a href="#ref-Brulhart2005" role="doc-biblioref">2005</a>)</span>.</p>
<p><span class="citation">Maasoumi (<a href="#ref-Maasoumi1993" role="doc-biblioref">1993</a>)</span> explicite cet usage et pour comparer la distribution de probabilité observée <span class="math inline">\(\{q_s\}\)</span> à une distribution attendue <span class="math inline">\(\{p_s\}\)</span>, plutôt qu’à la distribution uniforme dont toutes les valeurs seraient <span class="math inline">\(\frac{1}{S}\)</span>. En posant <span class="math inline">\(r=q-1\)</span>, en transformant les abondances <span class="math inline">\(\{n_s\}\)</span> en probabilités <span class="math inline">\(\{q_s\}\)</span> et en choisissant <span class="math inline">\(\{p_s\}\)</span> arbitrairement, on obtient</p>
<p><span class="math display" id="eq:Maasoumi1993">\[\begin{equation}
  \tag{4.36}
  H_r\left(\mathbf{q},\mathbf{p}\right) = \frac{1}{r\left(r+1\right)}\left[\sum^S_{s=1}{q_s{\left(\frac{q_s}{p_s}\right)}^{r}}-1\right].
\end{equation}\]</span></p>
<p><span class="citation">Cressie et Read (<a href="#ref-Cressie1984" role="doc-biblioref">1984</a>)</span> ont défini une mesure généralisée de qualité d’ajustement (<em>Goodness of Fit</em>) de la distribution <span class="math inline">\(\{q_s\}\)</span> à celle de <span class="math inline">\(\{p_s\}\)</span>.
Elle est identique à l’entropie généralisée (à un coefficient 2 près) bien que développée indépendamment pour d’autres motivations.
La statistique de Cressie et Read d’ordre 1 est la divergence de <span class="citation">Kullback et Leibler (<a href="#ref-Kullback1951" role="doc-biblioref">1951</a>)</span>, celle d’ordre 2 le <span class="math inline">\(\chi^2\)</span> de <span class="citation">Pearson (<a href="#ref-Pearson1900" role="doc-biblioref">1900</a>)</span>.</p>
<p><span class="citation">Studeny et al. (<a href="#ref-Studeny2011" role="doc-biblioref">2011</a>)</span> définissent une mesure d’équitabilité comme la divergence entre la distribution observée <span class="math inline">\(\{p_s\}\)</span> et la distribution uniforme, ce qui les ramène à l’entropie de Cowell:</p>
<p><span class="math display" id="eq:Studeny2011">\[\begin{equation}
  \tag{4.37}
  I_r 
  = \frac{1}{r\left(r+1\right)} \left[\sum^S_{s=1}{p_s \left(\frac{p_s}{1/S} \right)^{r}-1 } \right].
\end{equation}\]</span></p>
<p>Cette mesure inclut à la fois l’équitabilité et la richesse de la distribution.
Il s’agit donc d’une mesure de diversité, qui est, à une normalisation près, l’écart entre la valeur de l’entropie HCDT d’ordre <span class="math inline">\(q=r+1\)</span> et sa valeur maximale.
Elle généralise donc l’indice de <span class="citation">Theil (<a href="#ref-Theil1967" role="doc-biblioref">1967</a>)</span>:</p>
<p><span class="math display" id="eq:TheilGen">\[\begin{equation}
  \tag{4.38}
  I_r = \frac{1}{q S^{1-q}}\left( \ln_q{S} - ^{q}\!H \right).
\end{equation}\]</span></p>
</div>
<div id="sec-SimpsonG" class="section level3">
<h3><span class="header-section-number">4.7.3</span> L’entropie de Simpson généralisée</h3>
<p>L’entropie de Simpson généralisée a été introduite dans une classe d’indices plus générale <a href="chap-MesuresNeutres.html#eq:zeta">(3.38)</a> par <span class="citation">Zhang et Zhou (<a href="#ref-Zhang2010" role="doc-biblioref">2010</a>)</span> et étudiée en détail par <span class="citation">Zhang et Grabchak (<a href="#ref-Zhang2014" role="doc-biblioref">2016</a>)</span>.
L’entropie d’ordre <span class="math inline">\(r&gt;0\)</span> est
<span class="math display" id="eq:zetar">\[\begin{equation}
  \tag{4.39}
  \zeta_r = \sum_{s=1}^S p_s (1-p_s)^r.
\end{equation}\]</span></p>
<p>Elle se réduit à l’entropie de Simpson pour <span class="math inline">\(r=1\)</span>.</p>
<p>La fonction d’information de l’entropie de Simpson généralisée <span class="math inline">\(I(p)=(1-p)^r\)</span> représente la probabilité de n’observer aucun individu de l’espèce de probabilité <span class="math inline">\(p\)</span> dans un échantillon de taille <span class="math inline">\(r\)</span>: c’est une mesure intuitive de la rareté.
<span class="citation">Chao, Wang, et Jost (<a href="#ref-Chao2013" role="doc-biblioref">2013</a>)</span> ont interprété <span class="math inline">\(\zeta_r\)</span> comme la probabilité qu’un individu échantillonné au rang <span class="math inline">\(r+1\)</span> appartienne à une espèce nouvelle dans une SAC.
<span class="math inline">\((r+1)\zeta_r\)</span> est aussi l’espérance du nombre de singletons dans un échantillon de taille <span class="math inline">\((r+1)\)</span>.</p>
<p>L’intérêt majeur de cette entropie est qu’elle dispose d’un estimateur non biaisé pour toutes les valeurs de <span class="math inline">\(r\)</span> strictement inférieures à la taille de l’échantillon.
C’est une mesure de diversité valide pour les ordres <span class="math inline">\(r\)</span> strictement inférieurs au nombre d’espèces <span class="citation">(Grabchak et al. <a href="#ref-Grabchak2016" role="doc-biblioref">2017</a>)</span>.
Au-delà, elle ne respecte plus l’axiome d’équitabilité: sa valeur maximale n’est pas obtenue quand les espèces sont équiprobables.</p>
<p>Son nombre effectif d’espèces est
<span class="math display" id="eq:Dzeta">\[\begin{equation}
  \tag{4.40}
  ^{r}\!D^{\zeta} = \frac{1}{1 - \zeta_r^{\frac{1}{r}}}.
\end{equation}\]</span></p>
<p>L’existence d’un estimateur sans biais et d’un intervalle de confiance calculable analytiquement permet de comparer des profils de diversité de façon robuste quand l’échantillonnage est limité et la richesse élevée.</p>
<p>La figure <a href="entropie.html#fig:comProfilzetaFig">4.8</a>a montre les profils de diversité (en nombre effectifs d’espèces) des deux parcelles 6 et 18 de Paracou: 1 ha inventorié, respectivement 681 et 483 arbres, pour une richesse estimée à 254 et 309 espèces par les estimateurs jackknife d’ordres 2 et 3:</p>
<p></p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="entropie.html#cb165-1"></a>NsP6 &lt;-<span class="st"> </span><span class="kw">as.AbdVector</span>(Paracou618.MC<span class="op">$</span>Nsi[, <span class="dv">1</span>])</span>
<span id="cb165-2"><a href="entropie.html#cb165-2"></a>(S6 &lt;-<span class="st"> </span><span class="kw">Richness</span>(NsP6, <span class="dt">Correction =</span> <span class="st">&quot;Jackknife&quot;</span>))</span></code></pre></div>
<pre><code>## Jackknife 2 
##         254</code></pre>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="entropie.html#cb167-1"></a>NsP18 &lt;-<span class="st"> </span><span class="kw">as.AbdVector</span>(Paracou618.MC<span class="op">$</span>Nsi[, <span class="dv">2</span>])</span>
<span id="cb167-2"><a href="entropie.html#cb167-2"></a>(S18 &lt;-<span class="st"> </span><span class="kw">Richness</span>(NsP18, <span class="dt">Correction =</span> <span class="st">&quot;Jackknife&quot;</span>))</span></code></pre></div>
<pre><code>## Jackknife 3 
##         309</code></pre>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="entropie.html#cb169-1"></a>S &lt;-<span class="st"> </span><span class="kw">min</span>(S6, S18)</span></code></pre></div>
<p></p>
<p>En comparaison, la figure <a href="entropie.html#fig:comProfilzetaFig">4.8</a>b montre les profils de diversité HCDT des mêmes parcelles.
Ils se chevauchent pour les petits ordres de diversité, ce qui ne permet pas de conclure.</p>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:comProfilzetaFig"></span>
<img src="MesuresBD_files/figure-html/comProfilzetaFig-1.png" alt="Comparaison des profils de diversité des parcelles 6 et 18 de Paracou avec leur intervalle de confiance à 95%. La parcelle 18 est tracée en rouge, lignes continues, et la 6 en vert, lignes pointillées. (a) Diversité généralisée de Simpson (nombres effectif d’espèces): les profils sont distincts, ce qui permet de conclure que la parcelle 6 est plus diverse que la parcelle 18. (b) Diversité HCDT: les profils se chevauchent jusqu’à \(q \approx 0,7\)." width="100%" />
<p class="caption">
Figure 4.8: Comparaison des profils de diversité des parcelles 6 et 18 de Paracou avec leur intervalle de confiance à 95%. La parcelle 18 est tracée en rouge, lignes continues, et la 6 en vert, lignes pointillées. (a) Diversité généralisée de Simpson (nombres effectif d’espèces): les profils sont distincts, ce qui permet de conclure que la parcelle 6 est plus diverse que la parcelle 18. (b) Diversité HCDT: les profils se chevauchent jusqu’à <span class="math inline">\(q \approx 0,7\)</span>.
</p>
</div>
<p></p>
<p>La diversité est donc estimée jusqu’à l’ordre 253.</p>
<p>Code R pour réaliser la figure <a href="entropie.html#fig:comProfilzetaFig">4.8</a>:</p>
<p></p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="entropie.html#cb170-1"></a><span class="co"># Profil P6</span></span>
<span id="cb170-2"><a href="entropie.html#cb170-2"></a><span class="kw">library</span>(<span class="st">&quot;EntropyEstimation&quot;</span>)</span>
<span id="cb170-3"><a href="entropie.html#cb170-3"></a>zeta6 &lt;-<span class="st"> </span><span class="kw">CommunityProfile</span>(GenSimpson, NsP6, <span class="dv">1</span><span class="op">:</span>(S <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb170-4"><a href="entropie.html#cb170-4"></a>sigma6 &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>(S <span class="op">-</span><span class="st"> </span><span class="dv">1</span>), <span class="cf">function</span>(r) <span class="kw">GenSimp.sd</span>(NsP6, r))</span>
<span id="cb170-5"><a href="entropie.html#cb170-5"></a>ic6 &lt;-<span class="st"> </span><span class="kw">qnorm</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>sigma6<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(NsP6))</span>
<span id="cb170-6"><a href="entropie.html#cb170-6"></a>zeta6<span class="op">$</span>low &lt;-<span class="st"> </span>zeta6<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>ic6</span>
<span id="cb170-7"><a href="entropie.html#cb170-7"></a>zeta6<span class="op">$</span>high &lt;-<span class="st"> </span>zeta6<span class="op">$</span>y <span class="op">+</span><span class="st"> </span>ic6</span>
<span id="cb170-8"><a href="entropie.html#cb170-8"></a><span class="co"># Profil P18</span></span>
<span id="cb170-9"><a href="entropie.html#cb170-9"></a>zeta18 &lt;-<span class="st"> </span><span class="kw">CommunityProfile</span>(GenSimpson, NsP18, <span class="dv">1</span><span class="op">:</span>(S <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb170-10"><a href="entropie.html#cb170-10"></a>sigma18 &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>(S <span class="op">-</span><span class="st"> </span><span class="dv">1</span>), <span class="cf">function</span>(r) <span class="kw">GenSimp.sd</span>(NsP18, r))</span>
<span id="cb170-11"><a href="entropie.html#cb170-11"></a>ic18 &lt;-<span class="st"> </span><span class="kw">qnorm</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>sigma18<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">sum</span>(NsP18))</span>
<span id="cb170-12"><a href="entropie.html#cb170-12"></a>zeta18<span class="op">$</span>low &lt;-<span class="st"> </span>zeta18<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>ic18</span>
<span id="cb170-13"><a href="entropie.html#cb170-13"></a>zeta18<span class="op">$</span>high &lt;-<span class="st"> </span>zeta18<span class="op">$</span>y <span class="op">+</span><span class="st"> </span>ic18</span>
<span id="cb170-14"><a href="entropie.html#cb170-14"></a><span class="co"># Transformation en diversité</span></span>
<span id="cb170-15"><a href="entropie.html#cb170-15"></a>zeta6D &lt;-<span class="st"> </span>zeta6</span>
<span id="cb170-16"><a href="entropie.html#cb170-16"></a>zeta6D<span class="op">$</span>y &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(zeta6<span class="op">$</span>y)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>zeta6<span class="op">$</span>x))</span>
<span id="cb170-17"><a href="entropie.html#cb170-17"></a>zeta6D<span class="op">$</span>low &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(zeta6<span class="op">$</span>low)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>zeta6<span class="op">$</span>x))</span>
<span id="cb170-18"><a href="entropie.html#cb170-18"></a>zeta6D<span class="op">$</span>high &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(zeta6<span class="op">$</span>high)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>zeta6<span class="op">$</span>x))</span>
<span id="cb170-19"><a href="entropie.html#cb170-19"></a>zeta18D &lt;-<span class="st"> </span>zeta18</span>
<span id="cb170-20"><a href="entropie.html#cb170-20"></a>zeta18D<span class="op">$</span>y &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(zeta18<span class="op">$</span>y)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>zeta18<span class="op">$</span>x))</span>
<span id="cb170-21"><a href="entropie.html#cb170-21"></a>zeta18D<span class="op">$</span>low &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(zeta18<span class="op">$</span>low)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>zeta18<span class="op">$</span>x))</span>
<span id="cb170-22"><a href="entropie.html#cb170-22"></a>zeta18D<span class="op">$</span>high &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(zeta18<span class="op">$</span>high)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>zeta18<span class="op">$</span>x))</span>
<span id="cb170-23"><a href="entropie.html#cb170-23"></a><span class="co"># Figure</span></span>
<span id="cb170-24"><a href="entropie.html#cb170-24"></a>gga &lt;-<span class="st"> </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb170-25"><a href="entropie.html#cb170-25"></a><span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(x, <span class="dt">ymin =</span> low, <span class="dt">ymax =</span> high), </span>
<span id="cb170-26"><a href="entropie.html#cb170-26"></a>              <span class="kw">as.data.frame.list</span>(zeta18D), <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb170-27"><a href="entropie.html#cb170-27"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(x, y), <span class="kw">as.data.frame.list</span>(zeta18D), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb170-28"><a href="entropie.html#cb170-28"></a><span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(x, <span class="dt">ymin =</span> low, <span class="dt">ymax =</span> high), </span>
<span id="cb170-29"><a href="entropie.html#cb170-29"></a>              <span class="kw">as.data.frame.list</span>(zeta6D), <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb170-30"><a href="entropie.html#cb170-30"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(x, y), <span class="kw">as.data.frame.list</span>(zeta6D), <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>) <span class="op">+</span></span>
<span id="cb170-31"><a href="entropie.html#cb170-31"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Ordre de diversité&quot;</span>, </span>
<span id="cb170-32"><a href="entropie.html#cb170-32"></a>       <span class="dt">y =</span> <span class="st">&quot;Diversité de Simpson généralisée&quot;</span>) <span class="op">+</span></span>
<span id="cb170-33"><a href="entropie.html#cb170-33"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">tag =</span> <span class="st">&quot;a&quot;</span>)</span>
<span id="cb170-34"><a href="entropie.html#cb170-34"></a><span class="co"># Calcul des profils HCDT</span></span>
<span id="cb170-35"><a href="entropie.html#cb170-35"></a>D6 &lt;-<span class="st"> </span><span class="kw">CommunityProfile</span>(Diversity, NsP6, <span class="dt">NumberOfSimulations =</span> <span class="dv">10</span>,</span>
<span id="cb170-36"><a href="entropie.html#cb170-36"></a><span class="dt">q.seq=</span>q.seq, <span class="dt">Correction =</span> <span class="st">&quot;UnveilJ&quot;</span>)</span>
<span id="cb170-37"><a href="entropie.html#cb170-37"></a>D18 &lt;-<span class="st"> </span><span class="kw">CommunityProfile</span>(Diversity, NsP18, <span class="dt">NumberOfSimulations =</span> <span class="dv">10</span>,</span>
<span id="cb170-38"><a href="entropie.html#cb170-38"></a><span class="dt">q.seq=</span>q.seq, <span class="dt">Correction =</span> <span class="st">&quot;UnveilJ&quot;</span>)</span>
<span id="cb170-39"><a href="entropie.html#cb170-39"></a><span class="co"># Figure</span></span>
<span id="cb170-40"><a href="entropie.html#cb170-40"></a>ggb &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x =</span> D6<span class="op">$</span>x, D6<span class="op">$</span>y, D18<span class="op">$</span>y)) <span class="op">+</span></span>
<span id="cb170-41"><a href="entropie.html#cb170-41"></a><span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(x, <span class="dt">ymin =</span> low, <span class="dt">ymax =</span> high), <span class="kw">as.data.frame.list</span>(D6), </span>
<span id="cb170-42"><a href="entropie.html#cb170-42"></a>              <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb170-43"><a href="entropie.html#cb170-43"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(x, D6.y), <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb170-44"><a href="entropie.html#cb170-44"></a><span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(x, <span class="dt">ymin =</span> low, <span class="dt">ymax =</span> high), <span class="kw">as.data.frame.list</span>(D18), </span>
<span id="cb170-45"><a href="entropie.html#cb170-45"></a>              <span class="dt">alpha =</span> <span class="fl">0.3</span>) <span class="op">+</span></span>
<span id="cb170-46"><a href="entropie.html#cb170-46"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(x, D18.y), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb170-47"><a href="entropie.html#cb170-47"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Ordre de diversité&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Nombre de Hill&quot;</span>) <span class="op">+</span></span>
<span id="cb170-48"><a href="entropie.html#cb170-48"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">tag =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb170-49"><a href="entropie.html#cb170-49"></a><span class="kw">grid.arrange</span>(gga, ggb, <span class="dt">ncol =</span> <span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p>La distribution statistique de la différence d’entropie est connue, et peut donc être tracée (figure <a href="entropie.html#fig:zetaDiffFig">4.9</a>).
Son intervalle de confiance ne contient jamais la valeur 0, ce qui permet de conclure que la parcelle 18 est plus diverse.</p>

<p></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:zetaDiffFig"></span>
<img src="MesuresBD_files/figure-html/zetaDiffFig-1.png" alt="Différence entre les entropies de Simpson généralisées des parcelles 18 et 6 de Paracou avec son intervalle de confiance à 95%. La ligne horizontale représente l’hypothèse nulle d’égalité entre les entropies, qui peut être rejetée." width="80%" />
<p class="caption">
Figure 4.9: Différence entre les entropies de Simpson généralisées des parcelles 18 et 6 de Paracou avec son intervalle de confiance à 95%. La ligne horizontale représente l’hypothèse nulle d’égalité entre les entropies, qui peut être rejetée.
</p>
</div>
<p></p>
<p>Code R pour réaliser la figure:</p>
<p></p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="entropie.html#cb171-1"></a><span class="co"># Calcul de P18-P6 avec IC</span></span>
<span id="cb171-2"><a href="entropie.html#cb171-2"></a>  Difference &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">x =</span> zeta18<span class="op">$</span>x, <span class="dt">y =</span> zeta18<span class="op">$</span>y<span class="op">-</span>zeta6<span class="op">$</span>y)</span>
<span id="cb171-3"><a href="entropie.html#cb171-3"></a>  <span class="kw">class</span>(Difference) &lt;-<span class="st"> </span><span class="kw">class</span>(zeta18)</span>
<span id="cb171-4"><a href="entropie.html#cb171-4"></a>  icDifference &lt;-<span class="st"> </span><span class="kw">qnorm</span>(<span class="dv">1</span><span class="fl">-0.05</span><span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(sigma6<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(NsP6) </span>
<span id="cb171-5"><a href="entropie.html#cb171-5"></a>                                       <span class="op">+</span><span class="st"> </span>sigma18<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(NsP18))</span>
<span id="cb171-6"><a href="entropie.html#cb171-6"></a>  Difference<span class="op">$</span>low &lt;-<span class="st"> </span>Difference<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>icDifference</span>
<span id="cb171-7"><a href="entropie.html#cb171-7"></a>  Difference<span class="op">$</span>high &lt;-<span class="st"> </span>Difference<span class="op">$</span>y <span class="op">+</span><span class="st"> </span>icDifference</span>
<span id="cb171-8"><a href="entropie.html#cb171-8"></a>  <span class="co"># Plot</span></span>
<span id="cb171-9"><a href="entropie.html#cb171-9"></a>  <span class="kw">autoplot</span>(Difference, <span class="dt">xlab =</span> <span class="st">&quot;Ordre de diversité&quot;</span>, </span>
<span id="cb171-10"><a href="entropie.html#cb171-10"></a>           <span class="dt">ylab =</span> <span class="st">&quot;Différence d&#39;entropie&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb171-11"><a href="entropie.html#cb171-11"></a><span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p></p>
<p>La prise en compte des espèces rares est limitée par la valeur maximale de l’ordre <span class="math inline">\(r\)</span> (limité à <span class="math inline">\(S-1\)</span>).
Les espèces plus rares ne peuvent pas être prises en compte: le profil de diversité s’arrête là.
Le nombre effectif d’espèces maximum correspond au nombre de Hill d’ordre 0,5 environ.
Les espèces plus rares sont prises en compte dans la partie du profil de diversité HCDT d’ordre inférieur à 0,5 mais son incertitude est grande et son biais très important quand l’échantillonnage est aussi réduit <span class="citation">(Marcon <a href="#ref-Marcon2015a" role="doc-biblioref">2015</a>)</span>.</p>
<p>L’entropie de Simpson généralisée est donc l’outil le plus approprié pour comparer des profils de diversité de communautés riches et sous-échantillonnées.</p>
</div>
<div id="lentropie-par-cas" class="section level3">
<h3><span class="header-section-number">4.7.4</span> L’entropie par cas</h3>
<p><span class="citation">Rajaram et Castellani (<a href="#ref-Rajaram2016" role="doc-biblioref">2016</a>)</span> décomposent la diversité de Shannon en produit des contributions de chaque catégorie et proposent une mesure cumulative de la diversité d’une communauté prenant en compte une partie des espèces classées selon un critère pertinent, décrivant leur complexité: par exemple, la diversité des mammifères classés par taille croissante, ou la position dans l’évolution.</p>
<p>La fraction de la communauté dont le cumul des probabilités égale <span class="math inline">\(c\)</span> est prise en compte.
La diversité cumulée est:
<span class="math display" id="eq:Rajaram2016Dc">\[\begin{equation}
  \tag{4.41}
  \mathit{D_c} = \prod_c{p_s^{-p_s}}.
\end{equation}\]</span></p>
<p>La mesure normalisée est appelée <em>case-based entropy</em> en anglais et vaut
<span class="math display" id="eq:Rajaram2016Cc">\[\begin{equation}
  \tag{4.42}
  \mathit{C_c} = \frac{100 D_c}{^{1}\!D}.
\end{equation}\]</span></p>
<p><span class="math inline">\(\mathit{D_c}={^{1}\!D}\)</span> quand toute la communauté est prise en compte (<span class="math inline">\(c=1\)</span>).
Les auteurs s’intéressent principalement à la courbe de <span class="math inline">\(C_c\)</span> en fonction de <span class="math inline">\(c\)</span> pour montrer <span class="citation">(Castellani et Rajaram <a href="#ref-Castellani2016" role="doc-biblioref">2016</a>)</span> que la contribution à la diversité totale des 40% des composants les moints complexes de systèmes très divers (“des galaxies au gènes”) atteint toujours au moins 60%, ce résultat étant interprété comme une loi universelle de limitation de la complexité.</p>
<p>Cette mesure peut être étendue à l’entropie HCDT sans difficulté:
<span class="math display" id="eq:Ccq">\[\begin{equation}
  \tag{4.43}
  \mathit{^{q}\!C_{c}} = {\left(\sum_c{p^q_s}\right)}^{\frac{1}{1-q}}.
\end{equation}\]</span></p>
<p>La méthode peut être appliquée pour évaluer la contribution de n’importe quel partie de la communauté à la diversité totale.
Il ne s’agit pas d’une décomposition de la diversité au sens classique du terme, qui consiste à partitionner la diversité totale en diversité intra et inter-groupes, mais plutôt d’une façon de prendre en compte les caractéristiques des espèces dans une approche non-neutre.</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Adelman1969">
<p>Adelman, M. A. 1969. « Comment on the "H" Concentration Measure as a Numbers-Equivalent ». <em>The Review of Economics and Statistics</em> 51 (1): 99‑101. <a href="https://doi.org/10.2307/1926955">https://doi.org/10.2307/1926955</a>.</p>
</div>
<div id="ref-Beck2009">
<p>Beck, Christian. 2009. « Generalised Information and Entropy Measures in Physics ». <em>Contemporary Physics</em> 50 (4): 495‑510. <a href="https://doi.org/10.1080/00107510902823517">https://doi.org/10.1080/00107510902823517</a>.</p>
</div>
<div id="ref-Berger1970">
<p>Berger, Wolfgang H., et Frances L. Parker. 1970. « Diversity of Planktonic Foraminifera in Deep-Sea Sediments ». <em>Science</em> 168 (3937): 1345‑7. <a href="https://doi.org/10.1126/science.168.3937.1345">https://doi.org/10.1126/science.168.3937.1345</a>.</p>
</div>
<div id="ref-Box1964">
<p>Box, G. E. P., et D. R. Cox. 1964. « An Analysis of Transformations ». <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 26 (2): 211‑52. <a href="https://doi.org/10.1111/j.2517-6161.1964.tb00553.x">https://doi.org/10.1111/j.2517-6161.1964.tb00553.x</a>.</p>
</div>
<div id="ref-Broms2014">
<p>Broms, Kristin M., Mevin B. Hooten, et Ryan M. Fitzpatrick. 2014. « Accounting for Imperfect Detection in Hill Numbers for Biodiversity Studies ». <em>Methods in Ecology and Evolution</em> 6 (1): 99‑108. <a href="https://doi.org/10.1111/2041-210X.12296">https://doi.org/10.1111/2041-210X.12296</a>.</p>
</div>
<div id="ref-Brose2003">
<p>Brose, Ulrich, Neo D. Martinez, et Richard J. Williams. 2003. « Estimating Species Richness: Sensitivity to Sample Coverage and Insensitivity to Spatial Patterns ». <em>Ecology</em> 84 (9): 2364‑77. <a href="https://doi.org/10.1890/02-0558">https://doi.org/10.1890/02-0558</a>.</p>
</div>
<div id="ref-Brulhart2005">
<p>Brülhart, Marius, et Rolf Traeger. 2005. « An Account of Geographic Concentration Patterns in Europe ». <em>Regional Science and Urban Economics</em> 35 (6): 597‑624. <a href="https://doi.org/10.1016/j.regsciurbeco.2004.09.002">https://doi.org/10.1016/j.regsciurbeco.2004.09.002</a>.</p>
</div>
<div id="ref-Burnham1979">
<p>Burnham, K. 1979. « Robust Estimation of Population Size When Capture Probabilities Vary among Animals ». <em>Ecology</em> 60 (5): 927‑36. <a href="https://doi.org/10.2307/1936861">https://doi.org/10.2307/1936861</a>.</p>
</div>
<div id="ref-Castellani2016">
<p>Castellani, Brian, et Rajeev Rajaram. 2016. « Past the Power Law: Complex Systems and the Limiting Law of Restricted Diversity ». <em>Complexity</em> 21 (S2): 99‑112. <a href="https://doi.org/10.1002/cplx.21786">https://doi.org/10.1002/cplx.21786</a>.</p>
</div>
<div id="ref-Chao2010">
<p>Chao, Anne, Chun-Huo Chiu, et Lou Jost. 2010. « Phylogenetic Diversity Measures Based on Hill Numbers ». <em>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</em> 365 (1558): 3599‑3609. <a href="https://doi.org/10.1098/rstb.2010.0272%20Supplementary">https://doi.org/10.1098/rstb.2010.0272 Supplementary</a>.</p>
</div>
<div id="ref-Chao2014">
<p>Chao, Anne, Nicholas J. Gotelli, T. C. Hsieh, Elizabeth L. Sander, K. H. Ma, Robert K. Colwell, et Aaron M. Ellison. 2014. « Rarefaction and Extrapolation with Hill Numbers: A Framework for Sampling and Estimation in Species Diversity Studies ». <em>Ecological Monographs</em> 84 (1): 45‑67. <a href="https://doi.org/10.1890/13-0133.1">https://doi.org/10.1890/13-0133.1</a>.</p>
</div>
<div id="ref-Chao2014c">
<p>Chao, Anne, T. C. Hsieh, Robin L. Chazdon, Robert K. Colwell, et Nicholas J. Gotelli. 2015. « Unveiling the Species-Rank Abundance Distribution by Generalizing Good-Turing Sample Coverage Theory ». <em>Ecology</em> 96 (5): 1189‑1201. <a href="https://doi.org/10.1890/14-0550.1">https://doi.org/10.1890/14-0550.1</a>.</p>
</div>
<div id="ref-Chao2012b">
<p>Chao, Anne, et Lou Jost. 2012. « Coverage-Based Rarefaction and Extrapolation: Standardizing Samples by Completeness Rather than Size ». <em>Ecology</em> 93 (12): 2533‑47. <a href="https://doi.org/10.1890/11-1952.1">https://doi.org/10.1890/11-1952.1</a>.</p>
</div>
<div id="ref-Chao2015">
<p>Chao, Anne, et Lou Jost. 2015. « Estimating Diversity and Entropy Profiles via Discovery Rates of New Species ». <em>Methods in Ecology and Evolution</em> 6 (8): 873‑82. <a href="https://doi.org/10.1111/2041-210X.12349">https://doi.org/10.1111/2041-210X.12349</a>.</p>
</div>
<div id="ref-Chao2003">
<p>Chao, Anne, et Tsung-Jen Shen. 2003. « Nonparametric Estimation of Shannon’s Index of Diversity When There Are Unseen Species in Sample ». <em>Environmental and Ecological Statistics</em> 10 (4): 429‑43. <a href="https://doi.org/10.1023/A:1026096204727">https://doi.org/10.1023/A:1026096204727</a>.</p>
</div>
<div id="ref-Chao2013">
<p>Chao, Anne, Yi-Ting Wang, et Lou Jost. 2013. « Entropy and the Species Accumulation Curve: A Novel Entropy Estimator via Discovery Rates of New Species ». <em>Methods in Ecology and Evolution</em> 4 (11): 1091‑1100. <a href="https://doi.org/10.1111/2041-210x.12108">https://doi.org/10.1111/2041-210x.12108</a>.</p>
</div>
<div id="ref-Conceicao2000">
<p>Conceição, Pedro, et Pedro Ferreira. 2000. « The Young Person’s Guide to the Theil Index: Suggesting Intuitive Interpretations and Exploring Analytical Applications ». Austin, Texas. <a href="http://ssrn.com/paper=228703">http://ssrn.com/paper=228703</a>.</p>
</div>
<div id="ref-Cressie1984">
<p>Cressie, Noel, et Timothy R. C. Read. 1984. « Multinomial Goodness-of-Fit Tests ». <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 46 (3): 440‑64. <a href="http://www.jstor.org/stable/2345686">http://www.jstor.org/stable/2345686</a>.</p>
</div>
<div id="ref-Dalton1920">
<p>Dalton, Hugh. 1920. « The Measurement of the Inequality of Incomes ». <em>The Economic Journal</em> 30 (119): 348‑61. <a href="https://doi.org/10.2307/2223525">https://doi.org/10.2307/2223525</a>.</p>
</div>
<div id="ref-Daroczy1970">
<p>Daróczy, Zoltán. 1970. « Generalized Information Functions ». <em>Information and Control</em> 16 (1): 36‑51. <a href="https://doi.org/10.1016/s0019-9958(70)80040-7">https://doi.org/10.1016/s0019-9958(70)80040-7</a>.</p>
</div>
<div id="ref-Dauby2012">
<p>Dauby, Gilles, et Olivier J. Hardy. 2012. « Sampled-Based Estimation of Diversity Sensu Stricto by Transforming Hurlbert Diversities into Effective Number of Species ». <em>Ecography</em> 35 (7): 661‑72. <a href="https://doi.org/10.1111/j.1600-0587.2011.06860.x">https://doi.org/10.1111/j.1600-0587.2011.06860.x</a>.</p>
</div>
<div id="ref-Davis1941">
<p>Davis, H. T. 1941. <em>The Theory of Econometrics</em>. Bloomington, Indiana: The Principia Press.</p>
</div>
<div id="ref-Fattorini1999">
<p>Fattorini, L., et M. Marcheselli. 1999. « Inference on Intrinsic Diversity Profiles of Biological Populations ». <em>Environmetrics</em> 10 (5): 589‑99. <a href="https://doi.org/10.1002/(SICI)1099-095X(199909/10)10:5%3C589::AID-ENV374%3E3.0.CO;2-0">https://doi.org/10.1002/(SICI)1099-095X(199909/10)10:5&lt;589::AID-ENV374&gt;3.0.CO;2-0</a>.</p>
</div>
<div id="ref-Gadagkar1989">
<p>Gadagkar, Raghavendra. 1989. « An Undesirable Property of Hill’s Diversity Index N2 ». <em>Oecologia</em> 80: 140‑41. <a href="https://doi.org/10.1007/BF00789944">https://doi.org/10.1007/BF00789944</a>.</p>
</div>
<div id="ref-Good1953">
<p>Good, I. J. 1953. « The Population Frequency of Species and the Estimation of Population Parameters ». <em>Biometrika</em> 40 (3/4): 237‑64. <a href="https://doi.org/10.1093/biomet/40.3-4.237">https://doi.org/10.1093/biomet/40.3-4.237</a>.</p>
</div>
<div id="ref-Gotelli2001">
<p>Gotelli, Nicholas J., et Robert K. Colwell. 2001. « Quantifying Biodiversity: Procedures and Pitfalls in the Measurement and Comparison of Species Richness ». <em>Ecology Letters</em> 4 (4): 379‑91. <a href="https://doi.org/10.1046/j.1461-0248.2001.00230.x">https://doi.org/10.1046/j.1461-0248.2001.00230.x</a>.</p>
</div>
<div id="ref-Grabchak2016">
<p>Grabchak, Michael, Eric Marcon, Gabriel Lang, et Zhiyi Zhang. 2017. « The Generalized Simpson’s Entropy Is a Measure of Biodiversity ». <em>Plos One</em> 12 (3): e0173305. <a href="https://doi.org/10.1371/journal.pone.0173305">https://doi.org/10.1371/journal.pone.0173305</a>.</p>
</div>
<div id="ref-Grassberger1988">
<p>Grassberger, Peter. 1988. « Finite Sample Corrections to Entropy and Dimension Estimates ». <em>Physics Letters A</em> 128 (6-7): 369‑73. <a href="https://doi.org/10.1016/0375-9601(88)90193-4">https://doi.org/10.1016/0375-9601(88)90193-4</a>.</p>
</div>
<div id="ref-Gregorius1991">
<p>Gregorius, Hans-Rolf. 1991. « On the Concept of Effective Number ». <em>Theoretical population biology</em> 40 (2): 269‑83. <a href="https://doi.org/10.1016/0040-5809(91)90056-L">https://doi.org/10.1016/0040-5809(91)90056-L</a>.</p>
</div>
<div id="ref-Gregorius2010">
<p>Gregorius, Hans-Rolf. 2010. « Linking Diversity and Differentiation ». <em>Diversity</em> 2 (3): 370‑94. <a href="https://doi.org/10.3390/d2030370">https://doi.org/10.3390/d2030370</a>.</p>
</div>
<div id="ref-Gregorius2014">
<p>Gregorius, Hans-Rolf. 2014. « Partitioning of Diversity : The "within Communities" Component ». <em>Web Ecology</em> 14: 51‑60. <a href="https://doi.org/10.5194/we-14-51-2014">https://doi.org/10.5194/we-14-51-2014</a>.</p>
</div>
<div id="ref-Haegeman2013">
<p>Haegeman, Bart, Jérôme Hamelin, John Moriarty, Peter Neal, Jonathan Dushoff, et Joshua S Weitz. 2013. « Robust Estimation of Microbial Diversity in Theory and in Practice ». <em>The ISME journal</em> 7 (6): 1092‑1101. <a href="https://doi.org/10.1038/ismej.2013.10">https://doi.org/10.1038/ismej.2013.10</a>.</p>
</div>
<div id="ref-Havrda1967">
<p>Havrda, Jan, et František Charvát. 1967. « Quantification Method of Classification Processes. Concept of Structural Alpha-Entropy ». <em>Kybernetika</em> 3 (1): 30‑35. <a href="https://eudml.org/doc/28681">https://eudml.org/doc/28681</a>.</p>
</div>
<div id="ref-Hill1973">
<p>Hill, M. O. 1973. « Diversity and Evenness: A Unifying Notation and Its Consequences ». <em>Ecology</em> 54 (2): 427‑32. <a href="https://doi.org/10.2307/1934352">https://doi.org/10.2307/1934352</a>.</p>
</div>
<div id="ref-Hoffmann2008">
<p>Hoffmann, Sönke, et Andreas Hoffmann. 2008. « Is There a "True" Diversity? » <em>Ecological Economics</em> 65 (2): 213‑15. <a href="https://doi.org/10.1016/j.ecolecon.2008.01.009">https://doi.org/10.1016/j.ecolecon.2008.01.009</a>.</p>
</div>
<div id="ref-Hsieh2014">
<p>Hsieh, T. C., K. H. Ma, et Anne Chao. 2016. « iNEXT: An R Package for Interpolation and Extrapolation in Measuring Species Diversity ». <em>Methods in Ecology and Evolution</em> 7: 1451‑6. <a href="https://doi.org/10.1111/2041-210X.12613">https://doi.org/10.1111/2041-210X.12613</a>.</p>
</div>
<div id="ref-Hurlbert1971">
<p>Hurlbert, Stuart H. 1971. « The Nonconcept of Species Diversity: A Critique and Alternative Parameters ». <em>Ecology</em> 52 (4): 577‑86. <a href="https://doi.org/10.2307/1934145">https://doi.org/10.2307/1934145</a>.</p>
</div>
<div id="ref-Iknayan2014">
<p>Iknayan, Kelly J., Morgan W. Tingley, Brett J. Furnas, et Steven R. Beissinger. 2014. « Detecting Diversity: Emerging Methods to Estimate Species Diversity ». <em>Trends in Ecology &amp; Evolution</em> 29 (2): 97‑106. <a href="https://doi.org/10.1016/j.tree.2013.10.012">https://doi.org/10.1016/j.tree.2013.10.012</a>.</p>
</div>
<div id="ref-Jost2006">
<p>Jost, Lou. 2006. « Entropy and Diversity ». <em>Oikos</em> 113 (2): 363‑75. <a href="https://doi.org/10.1111/j.2006.0030-1299.14714.x">https://doi.org/10.1111/j.2006.0030-1299.14714.x</a>.</p>
</div>
<div id="ref-Jost2007">
<p>Jost, Lou. 2007. « Partitioning Diversity into Independent Alpha and Beta Components ». <em>Ecology</em> 88 (10): 2427‑39. <a href="https://doi.org/10.1890/06-1736.1">https://doi.org/10.1890/06-1736.1</a>.</p>
</div>
<div id="ref-Jost2009">
<p>Jost, Lou. 2009b. « Mismeasuring Biological Diversity: Response to Hoffmann and Hoffmann (2008) ». <em>Ecological Economics</em> 68: 925‑28. <a href="https://doi.org/10.1016/j.ecolecon.2008.10.015">https://doi.org/10.1016/j.ecolecon.2008.10.015</a>.</p>
</div>
<div id="ref-Keylock2005">
<p>Keylock, C. J. 2005. « Simpson Diversity and the Shannon-Wiener Index as Special Cases of a Generalized Entropy ». <em>Oikos</em> 109 (1): 203‑7. <a href="https://doi.org/10.1111/j.0030-1299.2005.13735.x">https://doi.org/10.1111/j.0030-1299.2005.13735.x</a>.</p>
</div>
<div id="ref-Kindt2006">
<p>Kindt, R., P. Van Damme, et A. J. Simons. 2006. « Tree Diversity in Western Kenya: Using Profiles to Characterise Richness and Evenness ». <em>Biodiversity and Conservation</em> 15 (4): 1253‑70. <a href="https://doi.org/10.1007/s10531-005-0772-x">https://doi.org/10.1007/s10531-005-0772-x</a>.</p>
</div>
<div id="ref-Kullback1951">
<p>Kullback, S., et R. A. Leibler. 1951. « On Information and Sufficiency ». <em>The Annals of Mathematical Statistics</em> 22 (1): 79‑86. <a href="http://www.jstor.org/stable/2236703">http://www.jstor.org/stable/2236703</a>.</p>
</div>
<div id="ref-Lande2000">
<p>Lande, Russell, Philip J. DeVries, et Thomas R. Walla. 2000. « When Species Accumulation Curves Intersect: Implications for Ranking Diversity Using Small Samples ». <em>Oikos</em> 89 (3): 601‑5. <a href="https://doi.org/10.1034/j.1600-0706.2000.890320.x">https://doi.org/10.1034/j.1600-0706.2000.890320.x</a>.</p>
</div>
<div id="ref-Leinster2012">
<p>Leinster, Tom, et Christina Cobbold. 2012. « Measuring Diversity: The Importance of Species Similarity ». <em>Ecology</em> 93 (3): 477‑89. <a href="https://doi.org/10.1890/10-2402.1">https://doi.org/10.1890/10-2402.1</a>.</p>
</div>
<div id="ref-Liu2006">
<p>Liu, Canran, Robert J. Whittaker, Keping Ma, et Jay R. Malcolm. 2006. « Unifying and Distinguishing Diversity Ordering Methods for Comparing Communities ». <em>Population Ecology</em> 49 (2): 89‑100. <a href="https://doi.org/10.1007/s10144-006-0026-0">https://doi.org/10.1007/s10144-006-0026-0</a>.</p>
</div>
<div id="ref-Maasoumi1993">
<p>Maasoumi, Esfandiar. 1993. « A Compendium to Information Theory in Economics and Econometrics ». <em>Econometric Reviews</em> 12 (2): 137‑81. <a href="https://doi.org/10.1080/07474939308800260">https://doi.org/10.1080/07474939308800260</a>.</p>
</div>
<div id="ref-MacArthur1955">
<p>MacArthur, Robert H. 1955. « Fluctuations of Animal Populations and a Measure of Community Stability ». <em>Ecology</em> 36 (3): 533‑36. <a href="https://doi.org/doi:10.2307/1929601">https://doi.org/doi:10.2307/1929601</a>.</p>
</div>
<div id="ref-MacArthur1965">
<p>MacArthur, Robert H. 1965. « Patterns of Species Diversity ». <em>Biological Reviews</em> 40 (4): 510‑33. <a href="https://doi.org/10.1111/j.1469-185X.1965.tb00815.x">https://doi.org/10.1111/j.1469-185X.1965.tb00815.x</a>.</p>
</div>
<div id="ref-Mao2007">
<p>Mao, Chang Xuan. 2007. « Estimating Species Accumulation Curves and Diversity Indices ». <em>Statistica Sinica</em> 17: 761‑74. <a href="http://www3.stat.sinica.edu.tw/statistica/oldpdf/A17n217.pdf">http://www3.stat.sinica.edu.tw/statistica/oldpdf/A17n217.pdf</a>.</p>
</div>
<div id="ref-Marcon2015a">
<p>Marcon, Eric. 2015. « Practical Estimation of Diversity from Abundance Data ». <em>HAL</em> 01212435 (version 2). <a href="https://hal-agroparistech.archives-ouvertes.fr/hal-01212435">https://hal-agroparistech.archives-ouvertes.fr/hal-01212435</a>.</p>
</div>
<div id="ref-Marcon2012a">
<p>Marcon, Eric, Bruno Hérault, Christopher Baraloto, et Gabriel Lang. 2012. « The Decomposition of Shannon’s Entropy and a Confidence Interval for Beta Diversity ». <em>Oikos</em> 121 (4): 516‑22. <a href="https://doi.org/10.1111/j.1600-0706.2011.19267.x">https://doi.org/10.1111/j.1600-0706.2011.19267.x</a>.</p>
</div>
<div id="ref-Marcon2014a">
<p>Marcon, Eric, Ivan Scotti, Bruno Hérault, Vivien Rossi, et Gabriel Lang. 2014. « Generalization of the Partitioning of Shannon Diversity ». <em>Plos One</em> 9 (3): e90289. <a href="https://doi.org/10.1371/journal.pone.0090289">https://doi.org/10.1371/journal.pone.0090289</a>.</p>
</div>
<div id="ref-Mendes2008">
<p>Mendes, R. S., L. R. Evangelista, S. M. Thomaz, A. A. Agostinho, et L. C. Gomes. 2008. « A Unified Index to Measure Ecological Diversity and Species Rarity ». <em>Ecography</em> 31 (4): 450‑56. <a href="https://doi.org/10.1111/j.0906-7590.2008.05469.x">https://doi.org/10.1111/j.0906-7590.2008.05469.x</a>.</p>
</div>
<div id="ref-Pallmann2012">
<p>Pallmann, Philip, Frank Schaarschmidt, Ludwig A. Hothorn, Christiane Fischer, Heiko Nacke, Kai U. Priesnitz, et Nicholas J. Schork. 2012. « Assessing Group Differences in Biodiversity by Simultaneously Testing a User-Defined Selection of Diversity Indices ». <em>Molecular Ecology Resources</em> 12 (6): 1068‑78. <a href="https://doi.org/10.1111/1755-0998.12004">https://doi.org/10.1111/1755-0998.12004</a>.</p>
</div>
<div id="ref-Patil1982">
<p>Patil, Ganapati P., et Charles Taillie. 1982. « Diversity as a Concept and Its Measurement ». <em>Journal of the American Statistical Association</em> 77 (379): 548‑61. <a href="https://doi.org/10.2307/2287709">https://doi.org/10.2307/2287709</a>.</p>
</div>
<div id="ref-Pearson1900">
<p>Pearson, Karl. 1900. « On the Criterion That a given System of Deviations from the Probable in the Case of a Correlated System of Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from Random Sampling ». <em>Philosophical Magazine Series 5</em> 50 (302): 157‑75. <a href="https://doi.org/10.1080/14786440009463897">https://doi.org/10.1080/14786440009463897</a>.</p>
</div>
<div id="ref-Pielou1975">
<p>Pielou, Evelyn C. 1975. <em>Ecological Diversity</em>. New York: Wiley.</p>
</div>
<div id="ref-Rajaram2016">
<p>Rajaram, R., et B. Castellani. 2016. « An Entropy Based Measure for Comparing Distributions of Complexity ». <em>Physica A: Statistical Mechanics and its Applications</em> 453: 35‑43. <a href="https://doi.org/10.1016/j.physa.2016.02.007">https://doi.org/10.1016/j.physa.2016.02.007</a>.</p>
</div>
<div id="ref-Renyi1961">
<p>Rényi, Alfréd. 1961. « On Measures of Entropy and Information ». In <em>4th Berkeley Symposium on Mathematical Statistics and Probability</em>, édité par Jerzy Neyman, 1:547‑61. Berkeley, USA: University of California Press.</p>
</div>
<div id="ref-Ricotta2005">
<p>Ricotta, Carlo. 2005c. « On Parametric Diversity Indices in Ecology: A Historical Note ». <em>Community Ecology</em> 6 (2): 241‑44. <a href="https://doi.org/10.1556/ComEc.6.2005.2.12">https://doi.org/10.1556/ComEc.6.2005.2.12</a>.</p>
</div>
<div id="ref-Ricotta2003c">
<p>Ricotta, Carlo, et Giancarlo C. 2003b. « An Information-Theoretical Measure of Taxonomic Diversity ». <em>Acta biotheoretica</em> 25 (51): 35‑41. <a href="https://doi.org/10.1023/A:1023000322071">https://doi.org/10.1023/A:1023000322071</a>.</p>
</div>
<div id="ref-Ricotta2006b">
<p>Ricotta, Carlo, et Laszlo Szeidl. 2006. « Towards a Unifying Approach to Diversity Measures: Bridging the Gap between the Shannon Entropy and Rao’s Quadratic Index ». <em>Theoretical Population Biology</em> 70 (3): 237‑43. <a href="https://doi.org/10.1016/j.tpb.2006.06.003">https://doi.org/10.1016/j.tpb.2006.06.003</a>.</p>
</div>
<div id="ref-Shannon1948">
<p>Shannon, Claude E. 1948. « A Mathematical Theory of Communication ». <em>The Bell System Technical Journal</em> 27 (3): 379‑423, 623‑56. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a>.</p>
</div>
<div id="ref-Shannon1963">
<p>Shannon, Claude E, et Warren Weaver. 1963. <em>The Mathematical Theory of Communication</em>. University of Illinois Press.</p>
</div>
<div id="ref-Stoddart1983">
<p>Stoddart, James A. 1983. « A Genotypic Diversity Measure ». <em>Journal of Heredity</em> 74: 489‑90. <a href="https://doi.org/10.1093/oxfordjournals.jhered.a109852">https://doi.org/10.1093/oxfordjournals.jhered.a109852</a>.</p>
</div>
<div id="ref-Studeny2011">
<p>Studeny, Angelika C., Stephen T. Buckland, Janine B. Illian, A. Johnston, et Anne E. Magurran. 2011. « Goodness-of-Fit Measures of Evenness: A New Tool for Exploring Changes in Community Structure ». <em>Ecosphere</em> 2 (2): art.15. <a href="https://doi.org/10.1890/ES10-00074.1">https://doi.org/10.1890/ES10-00074.1</a>.</p>
</div>
<div id="ref-Theil1967">
<p>Theil, Henri. 1967. <em>Economics and Information Theory</em>. Chicago: Rand McNally &amp; Company.</p>
</div>
<div id="ref-Tothmeresz1995">
<p>Tothmeresz, Béla. 1995. « Comparison of Different Methods for Diversity Ordering ». <em>Journal of Vegetation Science</em> 6 (2): 283‑90. <a href="https://doi.org/10.2307/3236223">https://doi.org/10.2307/3236223</a>.</p>
</div>
<div id="ref-Tsallis1988">
<p>Tsallis, Constantino. 1988. « Possible Generalization of Boltzmann-Gibbs Statistics ». <em>Journal of Statistical Physics</em> 52 (1): 479‑87. <a href="https://doi.org/10.1007/BF01016429">https://doi.org/10.1007/BF01016429</a>.</p>
</div>
<div id="ref-Tsallis1994">
<p>Tsallis, Constantino. 1994. « What Are the Numbers That Experiments Provide? » <em>Química Nova</em> 17 (6): 468‑71. <a href="http://quimicanova.sbq.org.br/detalhe_artigo.asp?id=5517">http://quimicanova.sbq.org.br/detalhe_artigo.asp?id=5517</a>.</p>
</div>
<div id="ref-Ulanowicz2001">
<p>Ulanowicz, Robert E. 2001. « Information Theory in Ecology ». <em>Computers &amp; Chemistry</em> 25 (4): 393‑99. <a href="https://doi.org/10.1016/S0097-8485(01)00073-0">https://doi.org/10.1016/S0097-8485(01)00073-0</a>.</p>
</div>
<div id="ref-Vinck2012">
<p>Vinck, Martin, Francesco P. Battaglia, Vladimir B. Balakirsky, A. J. Han Vinck, et Cyriel M. A. Pennartz. 2012. « Estimation of the Entropy Based on Its Polynomial Representation ». <em>Physical Review E</em> 85 (5). <a href="https://doi.org/10.1103/PhysRevE.85.051139">https://doi.org/10.1103/PhysRevE.85.051139</a>.</p>
</div>
<div id="ref-Wright1931">
<p>Wright, Sewall. 1931. « Evolution in Mendelian Populations ». <em>Genetics</em> 16 (2): 97‑159. <a href="https://www.genetics.org/content/16/2/97">https://www.genetics.org/content/16/2/97</a>.</p>
</div>
<div id="ref-Zhang2014c">
<p>Zhang, Jing, Thomas O. Crist, et Peijie Hou. 2014. « Partitioning of α and β Diversity Using Hierarchical Bayesian Modeling of Species Distribution and Abundance ». <em>Environmental and Ecological Statistics</em> 21 (4): 611‑25. <a href="https://doi.org/10.1007/s10651-013-0271-2">https://doi.org/10.1007/s10651-013-0271-2</a>.</p>
</div>
<div id="ref-Zhang2012">
<p>Zhang, Zhiyi. 2012. « Entropy Estimation in Turing’s Perspective ». <em>Neural Computation</em> 24 (5): 1368‑89. <a href="https://doi.org/10.1162/NECO_a_00266">https://doi.org/10.1162/NECO_a_00266</a>.</p>
</div>
<div id="ref-Zhang2014">
<p>Zhang, Zhiyi, et Michael Grabchak. 2016. « Entropic Representation and Estimation of Diversity Indices ». <em>Journal of Nonparametric Statistics</em> 28 (3): 563‑75. <a href="https://doi.org/10.1080/10485252.2016.1190357">https://doi.org/10.1080/10485252.2016.1190357</a>.</p>
</div>
<div id="ref-Zhang2010">
<p>Zhang, Zhiyi, et Jun Zhou. 2010. « Re-Parameterization of Multinomial Distributions and Diversity Indices ». <em>Journal of Statistical Planning and Inference</em> 140 (7): 1731‑8. <a href="https://doi.org/10.1016/j.jspi.2009.12.023">https://doi.org/10.1016/j.jspi.2009.12.023</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-MesuresNeutres.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-Equitabilite.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MesuresBD.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
